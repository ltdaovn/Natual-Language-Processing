{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was published at https://github.com/duyvuleo/VNTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will implement some algorithms to apply in text summarization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Text Summarization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text summarization is the problem of creating a short, accurate, and fluent summary of a longer text document.\n",
    "\n",
    "Automatic text summarization methods are greatly needed to address the ever-growing amount of text data available online to both better help discover relevant information and to consume relevant information faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will we do in this tutorial?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will solve Text Summarization for Vietnamese newspapers, using some algorithms belows:\n",
    "1. Extractive Text Summarization\n",
    "    - Doc2Vec\n",
    "    - Text Rank\n",
    "2. Abstractive Text Summarization\n",
    "    - Google textsum\n",
    "\n",
    "\n",
    "We just implement \"**Single document summarization**\" problem in this tutorial, another problem called \"**Multi-document summarization**\" will be dicussed in another time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic idea\n",
    "The idea of using Doc2Vec algorithm for text summarization problem is described as follows:\n",
    "1. In all documents, we will extract sentences separately.\n",
    "2. Each sentence will be represented by a vector, via doc2vec model\n",
    "3. Use KMean algorithm to find out most featured sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "dir_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "dir_path = os.path.join(dir_path, 'Data')\n",
    "\n",
    "sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_data(folder):\n",
    "    sentences = []\n",
    "    for path in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, path)\n",
    "        with open(file_path, 'r', encoding=\"utf-16\") as f:\n",
    "\n",
    "            lines = f.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                sens = line.split('.')\n",
    "                for sen in sens:\n",
    "                    if len(sen) > 10:\n",
    "                        sen = gensim.utils.simple_preprocess(sen)\n",
    "                        sen = ' '.join(sen)\n",
    "                        sen = ViTokenizer.tokenize(sen)\n",
    "                        sentences.append(sen)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sens = test_doc.split('.')\n",
    "# for sen in sens:\n",
    "#     if len(sen) > 10:\n",
    "#         sen = gensim.utils.simple_preprocess(sen)\n",
    "#         sen = ' '.join(sen)\n",
    "#         sen = ViTokenizer.tokenize(sen)\n",
    "#         sentences.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use multiprocessing here, but we will not use it for easy in understanding code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool\n",
    "# sentences = []\n",
    "# train_paths = [os.path.join(dir_path, 'VNTC-master/Data/10Topics/Ver1.1/Train_Full'), \n",
    "#                os.path.join(dir_path, 'VNTC-master/Data/10Topics/Ver1.1/Test_Full'),\n",
    "#                os.path.join(dir_path, 'VNTC-master/Data/27Topics/Ver1.1/new train'),\n",
    "#                os.path.join(dir_path, 'VNTC-master/Data/27Topics/Ver1.1/new test')]\n",
    "\n",
    "# dirs = []\n",
    "# for path in train_paths:\n",
    "#     for p in os.listdir(path):\n",
    "#         dirs.append(os.path.join(path, p))\n",
    "\n",
    "# for d in tqdm(dirs):\n",
    "#     sens = get_data(d)\n",
    "#     sentences = sentences + sens\n",
    "\n",
    "# # with Pool(8) as pool:\n",
    "# #     pool.map(get_data, tqdm(dirs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(sentences, open('./sentences.pkl', 'wb'))\n",
    "sentences = pickle.load(open('./sentences.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ông đồ cuối_cùng trên đảo'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(sentences):\n",
    "    corpus = []\n",
    "    \n",
    "    for i in tqdm(range(len(sentences))):\n",
    "        sen = sentences[i]\n",
    "        \n",
    "        words = sen.split(' ')\n",
    "        tagged_document = gensim.models.doc2vec.TaggedDocument(words, [i])\n",
    "        \n",
    "        corpus.append(tagged_document)\n",
    "        \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2385532/2385532 [00:34<00:00, 69769.71it/s]\n"
     ]
    }
   ],
   "source": [
    "train_corpus = get_corpus(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_corpus = shuffle(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=2, epochs=40)\n",
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# max_epochs = 40\n",
    "\n",
    "# for epoch in tqdm(range(max_epochs)):\n",
    "#     print('iteration {0}'.format(epoch))\n",
    "model.train(train_corpus[:50000],\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    \n",
    "#     # decrease the learning rate\n",
    "#     model.alpha -= 0.0002\n",
    "#     # fix the learning rate, no decay\n",
    "#     model.min_alpha = model.alpha\n",
    "\n",
    "# %time model.train(train_corpus[:50000], total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec.load('model/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04828287,  0.25527653,  1.1613333 , -0.43151897, -0.9858117 ,\n",
       "        0.10932952,  0.20315444, -0.48530903,  0.24952224, -0.11833256,\n",
       "       -0.0337567 , -0.3887124 , -0.39426357,  0.4454976 ,  0.64964545,\n",
       "       -0.5074249 ,  0.2037328 ,  0.32153234, -0.62261915,  0.8188216 ,\n",
       "        0.5820815 , -0.09879603, -0.44826344,  0.1201525 ,  0.236654  ,\n",
       "        0.13032307, -0.46023956,  0.19788027, -0.34569028, -0.21599784,\n",
       "        0.42319658, -0.106575  , -0.24495657, -0.00839793, -0.11475623,\n",
       "       -0.5559897 , -0.12046688,  0.18673038, -0.16149993, -0.02872676,\n",
       "        0.42999822,  0.46070522,  0.50624824, -0.15866163, -0.11092521,\n",
       "        0.30938515,  0.23203233,  0.11736044, -0.7434822 , -0.78674805,\n",
       "        0.27668393,  0.25058967, -0.15513541,  0.05721006, -0.62895125,\n",
       "       -0.3618494 ,  0.48457113, -0.16074707,  0.32852057, -0.63208133,\n",
       "       -0.45503548, -0.373764  ,  0.6417061 , -0.15453526,  0.828889  ,\n",
       "        0.4040729 , -0.13313939,  0.20088702, -0.36382645,  0.3100666 ,\n",
       "        0.02355373,  0.5920582 , -0.2271741 , -0.30618507, -0.23971866,\n",
       "        0.91544545, -0.51666105, -0.05829609, -0.43708014,  0.35457942,\n",
       "        0.50872976, -0.24838248,  0.44898847,  0.11512683,  0.34157744,\n",
       "       -0.47279087, -0.02090802,  0.23195563, -0.14476988,  0.5966468 ,\n",
       "        0.25278485,  0.70205003, -0.16960798, -0.09220067,  1.387285  ,\n",
       "        0.5248568 ,  0.33318955, -0.33651793,  0.41348195, -0.94656795,\n",
       "       -0.56593996,  0.6216159 ,  0.3179036 ,  0.31106716,  0.14830516,\n",
       "        0.535672  ,  0.695546  ,  0.28968796,  0.4329898 , -0.6800865 ,\n",
       "       -0.6313374 ,  0.36142987,  0.3392832 , -0.3685879 ,  1.0465527 ,\n",
       "       -0.31610152,  0.26410806, -0.75767416, -0.0933219 , -0.10084625,\n",
       "        0.11192366, -0.63711953,  0.6878306 ,  0.20774055,  0.37814376,\n",
       "       -0.38910306, -0.29257646,  0.32447788,  1.4432929 ,  0.42116693,\n",
       "        0.10012217, -0.54671454,  0.15930349, -0.04576634,  0.11046711,\n",
       "        0.4345503 ,  0.5950319 ,  0.10390531,  0.00534402, -0.05976183,\n",
       "        1.0111569 ,  0.14526764,  0.0051693 , -0.55909073,  0.18523502,\n",
       "       -0.59934396,  0.24894848, -0.18078412,  0.5796731 , -0.44970104,\n",
       "        0.81793183, -0.5046711 , -0.16381589,  0.14662668,  0.21144816,\n",
       "        0.08799265, -0.25188333, -0.39610714, -0.46737796,  0.06498595,\n",
       "       -0.24232577,  0.08590741, -0.34991795, -0.7811069 ,  0.05049568,\n",
       "       -0.44203833, -0.04051779, -0.93674725,  0.7014623 ,  0.43860036,\n",
       "        1.0785912 ,  0.4614321 ,  0.9178922 ,  0.01267096,  0.08151802,\n",
       "       -0.21591717, -0.389159  , -0.4332839 ,  0.06478307, -0.549585  ,\n",
       "        0.24735504, -0.15430401, -0.10635387,  0.9497028 , -0.5208101 ,\n",
       "       -0.25834572,  0.5067593 , -0.3163417 , -0.45160556, -1.0110141 ,\n",
       "       -0.11357957,  0.3088588 ,  0.67771375,  0.5347725 , -0.08545431,\n",
       "       -0.6260072 ,  0.37074357,  0.3511689 ,  0.03659426, -0.5359085 ,\n",
       "       -0.22255394, -0.4841223 , -0.31908542,  0.6693267 , -0.43263623,\n",
       "        0.17883465,  0.76907945,  0.3865581 , -0.27964267,  0.5833102 ,\n",
       "        0.10791489,  0.4569784 , -0.0223736 ,  0.48295155, -0.00460218,\n",
       "       -0.47181183, -0.48191187,  0.1006198 , -0.30717742,  0.62139356,\n",
       "        0.28134045,  0.29010874, -0.26925838,  0.8383542 , -0.18886985,\n",
       "        0.18526816, -0.57650745, -0.59799755,  0.19990733,  0.22144596,\n",
       "        0.70591587, -0.76111233,  0.13711332, -0.7318054 ,  0.02516509,\n",
       "       -0.3590674 , -0.6440488 , -0.5580956 , -0.5993928 , -0.32801956,\n",
       "       -0.4644991 ,  0.89624447, -0.39741072, -0.52681875, -0.29390556,\n",
       "       -0.3324342 , -0.62701875,  0.12948091,  0.9591448 , -0.21732959,\n",
       "       -0.6216343 , -0.04387471, -0.22252487,  0.27053964,  0.17134936,\n",
       "        0.69296885,  0.39905074,  0.3307731 , -0.38610834,  0.05903669,\n",
       "        0.40507847, -0.53825825,  0.08011609, -0.27195254, -0.296355  ,\n",
       "        0.27324116,  0.5513492 ,  0.77330786, -0.6397054 , -0.24681841,\n",
       "        0.2817206 ,  0.37891504,  0.03597298,  0.42222285, -0.06389087,\n",
       "        0.39442137,  0.07020057, -0.24582939,  0.279675  ,  0.00950517,\n",
       "       -0.60586107, -1.0425315 , -0.2628614 ,  0.20990998,  0.25524455,\n",
       "       -0.27130723,  0.51966363, -0.14886895,  0.8109764 ,  0.258794  ,\n",
       "       -0.05932726, -0.10472207,  0.06371555,  0.04762143,  0.02594266,\n",
       "       -1.0294654 , -0.5873498 ,  0.60305655, -0.07507906, -0.3711069 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(train_corpus[100000].words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = '''Trong trận bán kết lượt về AFF Cup 2018 diễn ra trên sân vận động Mỹ Đình tối 6/12, đội tuyển Việt Nam đã vượt qua đội tuyển Philippines với tỉ số 2-1. Qua đó, nâng tổng tỉ số sau hai lượt trận bán kết là 4-2.\n",
    "\n",
    "Đội tuyển Việt Nam đã xuất sắc giành quyền vào chơi trận chung kết AFF Cup sau tròn 10 năm chờ đợi. Đối thủ của chúng ta là đội tuyển Malaysia.\n",
    "\n",
    "Hai cầu thủ ghi bàn thắng trên sân Mỹ Đình tối qua là Quang Hải và Công Phượng. Đáng chú ý, bàn thắng của Công Phượng được ghi chỉ sau vài phút anh được HLV Park Hang Seo tung vào sân thay người ở những phút cuối cùng của trận đấu.\n",
    "\n",
    "Bàn thắng của Công Phượng không khỏi khiến nhiều người nhớ đến pha bỏ lỡ “không tưởng” của cầu thủ này ở trận bán kết lượt đi trên sân của đội tuyển Philippines hôm 2/12.\n",
    "\n",
    "Trong trận đấu ấy, Công Phượng cũng được HLV trưởng người Hàn Quốc tung vào sân ở những phút cuối trận đấu. Anh thực hiện một pha đi bóng qua hàng loạt cầu thủ hậu vệ Philippines. Thế nhưng, khi đối mặt với khung thành rộng lớn, anh lại sút bóng chệch cột dọc.\n",
    "\n",
    "Sau tình huống bỏ lỡ ấy, cộng đồng mạng Việt Nam thi nhau chế ảnh Công Phượng. Họ cho rằng, Công Phượng không chỉ lừa qua hàng loạt hậu vệ Philippines mà còn lừa luôn cả hàng triệu fan hâm mộ đội nhà.\n",
    "\n",
    "Thắng bán kết AFF Cup 2018, Công Phượng hết &#34;lừa&#34; fan, Văn Toàn hứa hẹn trở lại - 2\n",
    "\n",
    "Công Phượng đã không còn lừa người hâm mộ khi ghi bàn trong trận bán kết lượt về AFF Cup 2018.\n",
    "\n",
    "Chính vì vậy, trước trận đấu bán kết lượt về hôm qua, Công Phượng đã đăng tải một tấm hình lên mạng xã hội Facebook với tựa đề: “Ngày mai rồi đấy”.\n",
    "\n",
    "Dòng trạng thái ấy thể hiện quyết tâm của tiền đạo xứ Nghệ. Anh mong chờ được ra sân trong trận bán kết lượt về với Philippines để khẳng định mình và lấy lại niềm tin nơi người hâm mộ. Và cuối cùng, Công Phượng cũng đã làm được điều mình mong muốn.\n",
    "\n",
    "Thắng bán kết AFF Cup 2018, Công Phượng hết &#34;lừa&#34; fan, Văn Toàn hứa hẹn trở lại - 3\n",
    "\n",
    "Status trước hôm bán kết thể hiện sự quyết tâm của Công Phương.\n",
    "\n",
    "Ngay sau trận bán kết lượt về kết thúc, Công Phượng lại tiếp tục đăng một status: “Lần này không lừa cả nhà nữa nhé. Thắng rồi bà con ơi”. Với bàn thắng ghi được ở những phút cuối trận đấu, Công Phượng đã giúp đội tuyển Việt Nam chắc chắn vào chơi trận chung kết AFF Cup 2018.\n",
    "\n",
    "Cũng sau trận đấu bán kết lượt về khi đội tuyển Việt Nam vượt qua đội tuyển Philippines, cầu thủ Văn Toàn đã chia sẻ trạng thái: “Trở lại thôi”. Dòng trạng thái này của Văn Toàn như một thông điệp gửi tới người hâm mộ rằng, anh đã bình phục chấn thương và sẵn sàng trở lại ở trận chung kết.\n",
    "\n",
    "Thắng bán kết AFF Cup 2018, Công Phượng hết &#34;lừa&#34; fan, Văn Toàn hứa hẹn trở lại - 4\n",
    "\n",
    "Văn Toàn đăng status mang thông điệp đã bình phục chấn thương và sẵn sàng trở lại.\n",
    "\n",
    "Thắng bán kết AFF Cup 2018, Công Phượng hết &#34;lừa&#34; fan, Văn Toàn hứa hẹn trở lại - 5\n",
    "\n",
    "Người hâm mộ động viên tinh thần khi biết Văn Toàn sắp trở lại.\n",
    "\n",
    "Trước đó, Văn Toàn đã bị chấn thương sụn chêm ở đầu gối sau một pha va chạm với đồng đội Văn Quyết trong buổi tập trước trận đấu với đội tuyển Campuchia ở vòng bảng AFF Cup 2018.\n",
    "\n",
    "Rất may, chấn thương của Văn Toàn không quá nặng và không phải phẫu thuật nên bình phục nhanh chóng. Ban đầu, các bác sĩ của đội tuyển Việt Nam dự đoán Văn Toàn có thể trở lại ở trận bán kết lượt về. Tuy nhiên, chấn thương chưa bình phục hẳn nên Văn Toàn phải đợi đến chung kết để có cơ hội được ra sân.\n",
    "\n",
    "Những cầu thủ khác như Nguyễn Quang Hải, Phan Văn Đức, Phạm Đức Huy cũng có những chia sẻ lên Facebook cá nhân sau trận đấu. Các cầu thủ thầm cảm ơn những người thân, người hâm mộ đã luôn bên họ và chứng kiến họ trưởng thành.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trong trận bán kết lượt về AFF Cup 2018 diễn ra trên sân vận động Mỹ Đình tối 6/12, đội tuyển Việt Nam đã vượt qua đội tuyển Philippines với tỉ số 2-1. Qua đó, nâng tổng tỉ số sau hai lượt trận bán kết là 4-2.\\n\\nĐội tuyển Việt Nam đã xuất sắc giành quyền vào chơi trận chung kết AFF Cup sau tròn 10 năm chờ đợi. Đối thủ của chúng ta là đội tuyển Malaysia.\\n\\nHai cầu thủ ghi bàn thắng trên sân Mỹ Đình tối qua là Quang Hải và Công Phượng. Đáng chú ý, bàn thắng của Công Phượng được ghi chỉ sau vài phút anh được HLV Park Hang Seo tung vào sân thay người ở những phút cuối cùng của trận đấu.\\n\\nBàn thắng của Công Phượng không khỏi khiến nhiều người nhớ đến pha bỏ lỡ “không tưởng” của cầu thủ này ở trận bán kết lượt đi trên sân của đội tuyển Philippines hôm 2/12.\\n\\nTrong trận đấu ấy, Công Phượng cũng được HLV trưởng người Hàn Quốc tung vào sân ở những phút cuối trận đấu. Anh thực hiện một pha đi bóng qua hàng loạt cầu thủ hậu vệ Philippines. Thế nhưng, khi đối mặt với khung thành rộng lớn, anh lại sút bóng chệch cột dọc.\\n\\nSau tình huống bỏ lỡ ấy, cộng đồng mạng Việt Nam thi nhau chế ảnh Công Phượng. Họ cho rằng, Công Phượng không chỉ lừa qua hàng loạt hậu vệ Philippines mà còn lừa luôn cả hàng triệu fan hâm mộ đội nhà.\\n\\nThắng bán kết AFF Cup 2018, Công Phượng hết &#34;lừa&#34; fan, Văn Toàn hứa hẹn trở lại - 2\\n\\nCông Phượng đã không còn lừa người hâm mộ khi ghi bàn trong trận bán kết lượt về AFF Cup 2018.\\n\\nChính vì vậy, trước trận đấu bán kết lượt về hôm qua, Công Phượng đã đăng tải một tấm hình lên mạng xã hội Facebook với tựa đề: “Ngày mai rồi đấy”.\\n\\nDòng trạng thái ấy thể hiện quyết tâm của tiền đạo xứ Nghệ. Anh mong chờ được ra sân trong trận bán kết lượt về với Philippines để khẳng định mình và lấy lại niềm tin nơi người hâm mộ. Và cuối cùng, Công Phượng cũng đã làm được điều mình mong muốn.\\n\\nThắng bán kết AFF Cup 2018, Công Phượng hết &#34;lừa&#34; fan, Văn Toàn hứa hẹn trở lại - 3\\n\\nStatus trước hôm bán kết thể hiện sự quyết tâm của Công Phương.\\n\\nNgay sau trận bán kết lượt về kết thúc, Công Phượng lại tiếp tục đăng một status: “Lần này không lừa cả nhà nữa nhé. Thắng rồi bà con ơi”. Với bàn thắng ghi được ở những phút cuối trận đấu, Công Phượng đã giúp đội tuyển Việt Nam chắc chắn vào chơi trận chung kết AFF Cup 2018.\\n\\nCũng sau trận đấu bán kết lượt về khi đội tuyển Việt Nam vượt qua đội tuyển Philippines, cầu thủ Văn Toàn đã chia sẻ trạng thái: “Trở lại thôi”. Dòng trạng thái này của Văn Toàn như một thông điệp gửi tới người hâm mộ rằng, anh đã bình phục chấn thương và sẵn sàng trở lại ở trận chung kết.\\n\\nThắng bán kết AFF Cup 2018, Công Phượng hết &#34;lừa&#34; fan, Văn Toàn hứa hẹn trở lại - 4\\n\\nVăn Toàn đăng status mang thông điệp đã bình phục chấn thương và sẵn sàng trở lại.\\n\\nThắng bán kết AFF Cup 2018, Công Phượng hết &#34;lừa&#34; fan, Văn Toàn hứa hẹn trở lại - 5\\n\\nNgười hâm mộ động viên tinh thần khi biết Văn Toàn sắp trở lại.\\n\\nTrước đó, Văn Toàn đã bị chấn thương sụn chêm ở đầu gối sau một pha va chạm với đồng đội Văn Quyết trong buổi tập trước trận đấu với đội tuyển Campuchia ở vòng bảng AFF Cup 2018.\\n\\nRất may, chấn thương của Văn Toàn không quá nặng và không phải phẫu thuật nên bình phục nhanh chóng. Ban đầu, các bác sĩ của đội tuyển Việt Nam dự đoán Văn Toàn có thể trở lại ở trận bán kết lượt về. Tuy nhiên, chấn thương chưa bình phục hẳn nên Văn Toàn phải đợi đến chung kết để có cơ hội được ra sân.\\n\\nNhững cầu thủ khác như Nguyễn Quang Hải, Phan Văn Đức, Phạm Đức Huy cũng có những chia sẻ lên Facebook cá nhân sau trận đấu. Các cầu thủ thầm cảm ơn những người thân, người hâm mộ đã luôn bên họ và chứng kiến họ trưởng thành.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_sentence_vectors_from_document(doc, model):\n",
    "    vectors = []\n",
    "    sens = doc.split('.')\n",
    "    for sen in sens:\n",
    "        if len(sen) > 10:\n",
    "            sen = gensim.utils.simple_preprocess(sen)\n",
    "            sen = ' '.join(sen)\n",
    "            sen = ViTokenizer.tokenize(sen)\n",
    "            sen = sen.split(' ')\n",
    "            vec = model.infer_vector(sen)\n",
    "            \n",
    "            vectors.append(vec)\n",
    "    \n",
    "    return np.array(vectors), sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-2e6a5b890abc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msen_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_list_sentence_vectors_from_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "sen_vectors, sens = get_list_sentence_vectors_from_document(test_doc, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sen_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-8f05907afce0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msen_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sen_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "sen_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sen_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixture(covariance_type='full', init_params='kmeans', max_iter=100,\n",
       "        means_init=None, n_components=2, n_init=1, precisions_init=None,\n",
       "        random_state=None, reg_covar=1e-06, tol=0.001, verbose=0,\n",
       "        verbose_interval=10, warm_start=False, weights_init=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "n_clusters = 2\n",
    "\n",
    "gm = GaussianMixture(2)\n",
    "gm.fit(X)\n",
    "# kmeans = KMeans(n_clusters=n_clusters)\n",
    "# kmeans = kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07272727, 0.92727273])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Thắng bán kết AFF Cup 2018, Công Phượng hết &#34;lừa&#34; fan, Văn Toàn hứa hẹn trở lại - 5\n",
      "\n",
      "Người hâm mộ động viên tinh thần khi biết Văn Toàn sắp trở lại\n",
      "\n",
      "\n",
      "Hai cầu thủ ghi bàn thắng trên sân Mỹ Đình tối qua là Quang Hải và Công Phượng\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "avg = []\n",
    "for j in range(n_clusters):\n",
    "    idx = np.where(kmeans.labels_ == j)[0]\n",
    "    avg.append(np.mean(idx))\n",
    "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, X)\n",
    "ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "summary = [sens[closest[idx]] for idx in ordering]\n",
    "\n",
    "for sen in summary:\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(links):\n",
    "    website_list = links.keys()\n",
    "    return {website: index for index, website in enumerate(website_list)}\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "def build_transition_matrix(links, index):\n",
    "    total_links = 0\n",
    "    A = np.zeros((len(index), len(index)))\n",
    "    for webpage in links:\n",
    "        # dangling page\n",
    "        if not links[webpage]:\n",
    "            # Assign equal probabilities to transition to all the other pages\n",
    "            A[index[webpage]] = np.ones(len(index)) / len(index)\n",
    "        else:\n",
    "            for dest_webpage in links[webpage]:\n",
    "                total_links += 1\n",
    "                A[index[webpage]][index[dest_webpage]] = 1.0 / len(links[webpage])\n",
    " \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(A, eps=0.0001, d=0.85):\n",
    "    P = np.ones(len(A)) / len(A)\n",
    "    while True:\n",
    "        new_P = np.ones(len(A)) * (1 - d) / len(A) + d * A.T.dot(P)\n",
    "        delta = abs(new_P - P).sum()\n",
    "        if delta <= eps:\n",
    "            return new_P\n",
    "        P = new_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    " \n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix(sentences, stopwords=None):\n",
    "    # Create an empty similarity matrix\n",
    "    S = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    " \n",
    "            S[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    " \n",
    "    # normalize the matrix row-wise\n",
    "    for idx in range(len(S)):\n",
    "        S[idx] /= S[idx].sum()\n",
    " \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_sentences(doc):\n",
    "    sentences = []\n",
    "    sens = doc.split('.')\n",
    "    for sen in sens:\n",
    "        if len(sen) > 10:\n",
    "            sen = gensim.utils.simple_preprocess(sen)\n",
    "            sen = ' '.join(sen)\n",
    "            sen = ViTokenizer.tokenize(sen)\n",
    "            sen = sen.split(' ')\n",
    "#             print(sen)\n",
    "            sentences.append(sen)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-601637eeac0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_list_of_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-db93cc69b7d6>\u001b[0m in \u001b[0;36mget_list_of_sentences\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msen\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0msen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0msen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0msen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mViTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gensim' is not defined"
     ]
    }
   ],
   "source": [
    "sentences = get_list_of_sentences(test_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "S = build_similarity_matrix(sentences, stop_words)    \n",
    "# print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. bàn thắng của công phượng không khỏi khiến nhiều người nhớ đến pha bỏ lỡ không_tưởng của cầu_thủ này trận bán_kết lượt đi trên sân của đội_tuyển philippines hôm\n",
      "2. thắng bán_kết aff cup công phượng hết lừa fan văn_toàn hứa_hẹn trở_lại công phượng đã không còn lừa người hâm_mộ khi ghi_bàn trong trận bán_kết lượt về aff cup\n"
     ]
    }
   ],
   "source": [
    "def textrank(sentences, top_n=5, stopwords=None):\n",
    "    S = build_similarity_matrix(sentences, stop_words) \n",
    "    sentence_ranks = pagerank(S)\n",
    " \n",
    "    # Sort the sentence ranks\n",
    "    ranked_sentence_indexes = [item[0] for item in sorted(enumerate(sentence_ranks), key=lambda item: -item[1])]\n",
    "    selected_sentences = sorted(ranked_sentence_indexes[:top_n])\n",
    "    summary = itemgetter(*selected_sentences)(sentences)\n",
    "    return summary\n",
    " \n",
    "for idx, sentence in enumerate(textrank(sentences, top_n=2, stopwords=[])):\n",
    "    print(\"%s. %s\" % ((idx + 1), ' '.join(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
