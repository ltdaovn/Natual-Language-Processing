{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Vietnamese Newspaper Text Classifier 10 Topics.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kiXfolv5tfE"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p-GBaXn5tfK"
      },
      "source": [
        "The goal of text classification is to automatically classify the text documents into one or more defined categories. Some examples of text classification are:\n",
        "- Understanding audience sentiment from social media,\n",
        "- Detection of spam and non-spam emails,\n",
        "- Auto tagging of customer queries, and\n",
        "- Categorization of news articles into defined topics. <br> <br>\n",
        "\n",
        "Text Classification is an example of supervised machine learning task since a labelled dataset containing text documents and their labels is used for train a classifier. There are 4 steps that we need to do as follows:\n",
        "- Dataset Preparation (Preprocessing Data)\n",
        "- Feature Engineering (Preprocessing Data)\n",
        "- Model Training\n",
        "- Improve Performance \n",
        "\n",
        "\n",
        "In this tutorial, we will implement Text Classifier Model for newpapers in Vietnamese. <br>\n",
        "There are totally 10 classes in data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FK--FJt5tfM"
      },
      "source": [
        "# Preprocessing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfiWQLa15tfM"
      },
      "source": [
        "Dataset was downloaded from https://github.com/duyvuleo/VNTC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiFbnKrI6rvj"
      },
      "source": [
        "!wget -c https://github.com/duyvuleo/VNTC/raw/master/Data/10Topics/Ver1.1/Train_Full.rar\n",
        "!unrar x -r Train_Full.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HuIXFn0g5tfO"
      },
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "from keras.layers import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLgD-KOG5tfT"
      },
      "source": [
        "## Dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkj0iQZR5tfU",
        "outputId": "c4f15a98-9e79-4965-dc61-78a90148c987",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pyvi\n",
        "from pyvi import ViTokenizer, ViPosTagger\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import gensim\n",
        "import numpy as np"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 4.7 MB/s \n",
            "\u001b[?25hCollecting sklearn-crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyvi) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "  Downloading python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (4.62.0)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.7 pyvi-0.1.1 sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "M4WCUgM75tfW",
        "outputId": "b6da5111-a7eb-41d3-a465-413507a3af14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os \n",
        "\n",
        "def get_data(folder_path):\n",
        "    X = []\n",
        "    y = []\n",
        "    dirs = os.listdir(folder_path)\n",
        "    for path in dirs:\n",
        "        file_paths = os.listdir(os.path.join(folder_path, path))\n",
        "        for file_path in tqdm(file_paths):\n",
        "            with open(os.path.join(folder_path, path, file_path), 'r', encoding=\"utf-16\") as f:\n",
        "                lines = f.readlines()\n",
        "                lines = ' '.join(lines)\n",
        "                lines = gensim.utils.simple_preprocess(lines)\n",
        "                lines = ' '.join(lines)\n",
        "                lines = ViTokenizer.tokenize(lines)\n",
        "#                 sentence = ' '.join(words)\n",
        "#                 print(lines)\n",
        "                X.append(lines)\n",
        "                y.append(path)\n",
        "#             break\n",
        "#         break\n",
        "    return X, y\n",
        "\n",
        "#train_path = os.path.join(dir_path, 'Train_Full')\n",
        "X_data, y_data = get_data('Train_Full')\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1820/1820 [00:18<00:00, 98.36it/s]\n",
            "100%|██████████| 3159/3159 [00:43<00:00, 72.65it/s]\n",
            "100%|██████████| 5219/5219 [00:56<00:00, 91.96it/s] \n",
            "100%|██████████| 3080/3080 [00:36<00:00, 84.31it/s]\n",
            "100%|██████████| 2552/2552 [00:26<00:00, 97.75it/s]\n",
            "100%|██████████| 2481/2481 [00:21<00:00, 118.11it/s]\n",
            "100%|██████████| 2898/2898 [00:28<00:00, 102.62it/s]\n",
            "100%|██████████| 5298/5298 [01:07<00:00, 79.04it/s]\n",
            "100%|██████████| 3868/3868 [00:34<00:00, 112.76it/s]\n",
            "100%|██████████| 3384/3384 [00:33<00:00, 99.85it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcG3u0i95tfY"
      },
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(X_data, open('X_data.pkl', 'wb'))\n",
        "pickle.dump(y_data, open('y_data.pkl', 'wb'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1uTSf3SW5tfa"
      },
      "source": [
        "!wget -c https://github.com/ltdaovn/VNTC/raw/master/Data/10Topics/Ver1.1/Test_Full.rar\n",
        "!unrar x -r Test_Full.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOu1zt1TAxrt",
        "outputId": "cacaac22-aaa6-43ce-e2ad-caf56b8f2b89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#test_path = os.path.join(dir_path, 'VNTC-master/Data/10Topics/Ver1.1/Test_Full')\n",
        "X_test, y_test = get_data('Test_Full')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2096/2096 [00:25<00:00, 83.72it/s]\n",
            "100%|██████████| 2036/2036 [00:34<00:00, 58.19it/s]\n",
            "100%|██████████| 7567/7567 [01:20<00:00, 94.13it/s]\n",
            "100%|██████████| 6250/6250 [01:19<00:00, 78.52it/s]\n",
            "100%|██████████| 5276/5276 [01:00<00:00, 87.54it/s]\n",
            "100%|██████████| 4560/4560 [00:44<00:00, 101.35it/s]\n",
            "100%|██████████| 6716/6716 [01:01<00:00, 109.01it/s]\n",
            "100%|██████████| 6667/6667 [01:32<00:00, 72.25it/s]\n",
            "100%|██████████| 3788/3788 [00:36<00:00, 103.18it/s]\n",
            "100%|██████████| 5417/5417 [01:01<00:00, 88.09it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ca-K7DW5tfb"
      },
      "source": [
        "pickle.dump(X_test, open('X_test.pkl', 'wb'))\n",
        "pickle.dump(y_test, open('y_test.pkl', 'wb'))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3mUSzTk5tfb"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSbP2PT_5tfc"
      },
      "source": [
        "In this step, raw text data will be transformed into eature vectors and new features will be created using the existing dataset. We will implement some idea as follows:\n",
        "1. Count Vectors as features\n",
        "2. TF-IDF Vectors as features<br>\n",
        "    2.1. Word level<br>\n",
        "    2.2. N-Gram level<br>\n",
        "    2.3. Character level\n",
        "3. Word Embeddings as features\n",
        "4. Text / NLP based features\n",
        "5. Topic Models as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz6Z35We5tfc"
      },
      "source": [
        "import pickle\n",
        "\n",
        "X_data = pickle.load(open('X_data.pkl', 'rb'))\n",
        "y_data = pickle.load(open('y_data.pkl', 'rb'))\n",
        "\n",
        "X_test = pickle.load(open('X_test.pkl', 'rb'))\n",
        "y_test = pickle.load(open('y_test.pkl', 'rb'))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZy19GlE5tfd"
      },
      "source": [
        "### Count Vectors as features\n",
        "Count Vector is a matrix notation of the dataset in which every row represents a document from the corpus, every column represents a term from the corpus, and every cell represents the frequency count of a particular term in a particular document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KzX-eNj5tfd"
      },
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(X_data)\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "X_data_count = count_vect.transform(X_data)\n",
        "X_test_count = count_vect.transform(X_test)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hfgsC1G5tfe"
      },
      "source": [
        "### TF-IDF Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-_kqyXS5tfe"
      },
      "source": [
        "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)<br>\n",
        "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)<br>\n",
        "TF-IDF Vectors can be generated at different levels of input tokens (words, characters, n-grams)\n",
        "\n",
        "a. Word Level TF-IDF : Matrix representing tf-idf scores of every term in different documents\n",
        "\n",
        "b. N-gram Level TF-IDF : N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-grams\n",
        "\n",
        "c. Character Level TF-IDF : Matrix representing tf-idf scores of character level n-grams in the corpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-YLv_W55tff"
      },
      "source": [
        "# word level - we choose max number of words equal to 30000 except all words (100k+ words)\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)\n",
        "tfidf_vect.fit(X_data) # learn vocabulary and idf from training set\n",
        "X_data_tfidf =  tfidf_vect.transform(X_data)\n",
        "# assume that we don't have test set before\n",
        "X_test_tfidf =  tfidf_vect.transform(X_test)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "YiCgnc2L5tfg",
        "outputId": "c698b1e0-f3fe-4deb-862e-dd34fc7470d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tfidf_vect.get_feature_names()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aa',\n",
              " 'aaa',\n",
              " 'aac',\n",
              " 'aachen',\n",
              " 'aaron',\n",
              " 'aas',\n",
              " 'ab',\n",
              " 'aba',\n",
              " 'abashidze',\n",
              " 'abba',\n",
              " 'abbas',\n",
              " 'abbey',\n",
              " 'abbiati',\n",
              " 'abbondanzieri',\n",
              " 'abbott',\n",
              " 'abc',\n",
              " 'abd',\n",
              " 'abdel',\n",
              " 'abdelrahim',\n",
              " 'abdoulaye',\n",
              " 'abdul',\n",
              " 'abdulaziz',\n",
              " 'abdullah',\n",
              " 'abe',\n",
              " 'abel',\n",
              " 'aberdeen',\n",
              " 'abeyie',\n",
              " 'abf',\n",
              " 'abidjan',\n",
              " 'abkhazia',\n",
              " 'able',\n",
              " 'abn',\n",
              " 'about',\n",
              " 'abqaiq',\n",
              " 'abraham',\n",
              " 'abramoff',\n",
              " 'abramovich',\n",
              " 'abs',\n",
              " 'abtc',\n",
              " 'abu',\n",
              " 'ac',\n",
              " 'academy',\n",
              " 'acasiete',\n",
              " 'acb',\n",
              " 'acbs',\n",
              " 'accc',\n",
              " 'accept',\n",
              " 'access',\n",
              " 'account',\n",
              " 'accumbens',\n",
              " 'ace',\n",
              " 'aceh',\n",
              " 'acer',\n",
              " 'acetaminophen',\n",
              " 'achilefu',\n",
              " 'achilles',\n",
              " 'acid',\n",
              " 'acid_amin',\n",
              " 'acid_béo',\n",
              " 'acl',\n",
              " 'acm',\n",
              " 'acoo',\n",
              " 'acpe',\n",
              " 'acrobat',\n",
              " 'acronis',\n",
              " 'acropolis',\n",
              " 'acrylic',\n",
              " 'act',\n",
              " 'action',\n",
              " 'active',\n",
              " 'activex',\n",
              " 'acuff',\n",
              " 'acyclovir',\n",
              " 'ad',\n",
              " 'adam',\n",
              " 'adams',\n",
              " 'adan',\n",
              " 'adani',\n",
              " 'adapter',\n",
              " 'adb',\n",
              " 'add',\n",
              " 'address',\n",
              " 'addvote',\n",
              " 'adebayor',\n",
              " 'adelaide',\n",
              " 'adelman',\n",
              " 'aden',\n",
              " 'adeno',\n",
              " 'adeportivo',\n",
              " 'adidas',\n",
              " 'adler',\n",
              " 'admin',\n",
              " 'adn',\n",
              " 'adnan',\n",
              " 'adobe',\n",
              " 'adodb',\n",
              " 'adrenalin',\n",
              " 'adrenaline',\n",
              " 'adrian',\n",
              " 'adriano',\n",
              " 'adsl',\n",
              " 'adu',\n",
              " 'adulyadej',\n",
              " 'aduôm',\n",
              " 'advanced',\n",
              " 'adventure',\n",
              " 'adventures',\n",
              " 'advocaat',\n",
              " 'adware',\n",
              " 'adzharia',\n",
              " 'ae',\n",
              " 'aegypti',\n",
              " 'aek',\n",
              " 'aerobic',\n",
              " 'aerospace',\n",
              " 'af',\n",
              " 'afa',\n",
              " 'afc',\n",
              " 'afd',\n",
              " 'aff',\n",
              " 'affleck',\n",
              " 'afghanistan',\n",
              " 'afp',\n",
              " 'afta',\n",
              " 'after',\n",
              " 'ag',\n",
              " 'again',\n",
              " 'agamemnon',\n",
              " 'agassi',\n",
              " 'age',\n",
              " 'agf',\n",
              " 'agger',\n",
              " 'agifish',\n",
              " 'agnelli',\n",
              " 'agobot',\n",
              " 'agostino',\n",
              " 'agra',\n",
              " 'agresto',\n",
              " 'agribank',\n",
              " 'agricole',\n",
              " 'ags',\n",
              " 'agtek',\n",
              " 'agu',\n",
              " 'aguilar',\n",
              " 'aguilera',\n",
              " 'agus',\n",
              " 'agustin',\n",
              " 'ah',\n",
              " 'aha',\n",
              " 'ahern',\n",
              " 'ahmad',\n",
              " 'ahmadinejad',\n",
              " 'ahmed',\n",
              " 'ahn',\n",
              " 'ahv',\n",
              " 'ai',\n",
              " 'ai_ai',\n",
              " 'ai_bảo',\n",
              " 'ai_lại',\n",
              " 'ai_ngờ',\n",
              " 'ai_nấy',\n",
              " 'ai_đời',\n",
              " 'aia',\n",
              " 'aiboy',\n",
              " 'aid',\n",
              " 'aide',\n",
              " 'aids',\n",
              " 'aiko',\n",
              " 'ailen',\n",
              " 'ailton',\n",
              " 'aim',\n",
              " 'aimar',\n",
              " 'aime',\n",
              " 'ain',\n",
              " 'aio',\n",
              " 'air',\n",
              " 'airbus',\n",
              " 'aires',\n",
              " 'airlines',\n",
              " 'airways',\n",
              " 'ait',\n",
              " 'aitor',\n",
              " 'aiyegbeni',\n",
              " 'aj',\n",
              " 'ajax',\n",
              " 'ajc',\n",
              " 'ajinomoto',\n",
              " 'ak',\n",
              " 'akayev',\n",
              " 'akbar',\n",
              " 'akhtar',\n",
              " 'akinfeev',\n",
              " 'akira',\n",
              " 'akishino',\n",
              " 'al',\n",
              " 'ala',\n",
              " 'alabama',\n",
              " 'alain',\n",
              " 'alam',\n",
              " 'alamsyah',\n",
              " 'alan',\n",
              " 'alarm',\n",
              " 'alaska',\n",
              " 'alaves',\n",
              " 'alawi',\n",
              " 'alba',\n",
              " 'albacete',\n",
              " 'albania',\n",
              " 'albelda',\n",
              " 'albena',\n",
              " 'albert',\n",
              " 'alberta',\n",
              " 'albertini',\n",
              " 'alberto',\n",
              " 'albion',\n",
              " 'albright',\n",
              " 'album',\n",
              " 'albumin',\n",
              " 'alcatel',\n",
              " 'alcomto',\n",
              " 'aldo',\n",
              " 'aldonin',\n",
              " 'ale',\n",
              " 'alec',\n",
              " 'alegre',\n",
              " 'alejandro',\n",
              " 'aleksander',\n",
              " 'aleksandr',\n",
              " 'aleksandrs',\n",
              " 'alenichev',\n",
              " 'alert',\n",
              " 'alessandro',\n",
              " 'alessio',\n",
              " 'alex',\n",
              " 'alexander',\n",
              " 'alexandra',\n",
              " 'alexandre',\n",
              " 'alexandria',\n",
              " 'alexei',\n",
              " 'alexey',\n",
              " 'alexis',\n",
              " 'alezboo',\n",
              " 'alfa',\n",
              " 'alfonso',\n",
              " 'alfred',\n",
              " 'alfredo',\n",
              " 'algeria',\n",
              " 'ali',\n",
              " 'aliadiere',\n",
              " 'alice',\n",
              " 'alicia',\n",
              " 'alienware',\n",
              " 'alive',\n",
              " 'aliénor',\n",
              " 'alkmaar',\n",
              " 'all',\n",
              " 'alla',\n",
              " 'allah',\n",
              " 'allan',\n",
              " 'allardyce',\n",
              " 'allawi',\n",
              " 'allback',\n",
              " 'allen',\n",
              " 'allende',\n",
              " 'alley',\n",
              " 'alliance',\n",
              " 'allianz',\n",
              " 'almeida',\n",
              " 'almeyda',\n",
              " 'almunia',\n",
              " 'alofun',\n",
              " 'alonso',\n",
              " 'alpe',\n",
              " 'alpha',\n",
              " 'alphanam',\n",
              " 'alphonse',\n",
              " 'alpi',\n",
              " 'alps',\n",
              " 'alt',\n",
              " 'altavista',\n",
              " 'alternative',\n",
              " 'altintop',\n",
              " 'altis',\n",
              " 'alvarez',\n",
              " 'alvaro',\n",
              " 'alves',\n",
              " 'alvin',\n",
              " 'alwaleed',\n",
              " 'alzheimer',\n",
              " 'alô',\n",
              " 'am',\n",
              " 'am_hiểu',\n",
              " 'am_tường',\n",
              " 'ama',\n",
              " 'amador',\n",
              " 'aman',\n",
              " 'amanda',\n",
              " 'amantadine',\n",
              " 'amaobi',\n",
              " 'amara',\n",
              " 'amarilla',\n",
              " 'amaseco',\n",
              " 'amath',\n",
              " 'amazon',\n",
              " 'ambrose',\n",
              " 'ambrosini',\n",
              " 'ambrosio',\n",
              " 'amcham',\n",
              " 'amd',\n",
              " 'amedeo',\n",
              " 'amelia',\n",
              " 'amelie',\n",
              " 'ameobi',\n",
              " 'america',\n",
              " 'american',\n",
              " 'amie',\n",
              " 'amin',\n",
              " 'amino',\n",
              " 'amir',\n",
              " 'amiro',\n",
              " 'amiăng',\n",
              " 'amiđan',\n",
              " 'amjad',\n",
              " 'amma',\n",
              " 'amman',\n",
              " 'ammar',\n",
              " 'amoniac',\n",
              " 'amore',\n",
              " 'amoro',\n",
              " 'amoroso',\n",
              " 'amoruso',\n",
              " 'amour',\n",
              " 'amoxicillin',\n",
              " 'amp',\n",
              " 'ampad',\n",
              " 'ampli',\n",
              " 'amposted',\n",
              " 'amr',\n",
              " 'amri',\n",
              " 'amsterdam',\n",
              " 'amstrong',\n",
              " 'amsubject',\n",
              " 'amy',\n",
              " 'amíp',\n",
              " 'an',\n",
              " 'an_bài',\n",
              " 'an_cư',\n",
              " 'an_giang',\n",
              " 'an_hưởng',\n",
              " 'an_khang',\n",
              " 'an_khê',\n",
              " 'an_lành',\n",
              " 'an_lạc',\n",
              " 'an_nghỉ',\n",
              " 'an_nhàn',\n",
              " 'an_ninh',\n",
              " 'an_phận',\n",
              " 'an_sinh',\n",
              " 'an_thần',\n",
              " 'an_toàn',\n",
              " 'an_táng',\n",
              " 'an_tâm',\n",
              " 'an_ủi',\n",
              " 'ana',\n",
              " 'analog',\n",
              " 'analytics',\n",
              " 'anara',\n",
              " 'anastasia',\n",
              " 'anatoly',\n",
              " 'anbar',\n",
              " 'ancelotti',\n",
              " 'ancic',\n",
              " 'ancona',\n",
              " 'and',\n",
              " 'andaman',\n",
              " 'anderlecht',\n",
              " 'anders',\n",
              " 'andersen',\n",
              " 'anderson',\n",
              " 'andersson',\n",
              " 'anderton',\n",
              " 'andes',\n",
              " 'andorra',\n",
              " 'andrade',\n",
              " 'andre',\n",
              " 'andrea',\n",
              " 'andreas',\n",
              " 'andreev',\n",
              " 'andrei',\n",
              " 'andreina',\n",
              " 'andrejs',\n",
              " 'andres',\n",
              " 'andrew',\n",
              " 'andrews',\n",
              " 'andrey',\n",
              " 'andriy',\n",
              " 'androgen',\n",
              " 'andré',\n",
              " 'andy',\n",
              " 'anelka',\n",
              " 'anfield',\n",
              " 'ang',\n",
              " 'angel',\n",
              " 'angela',\n",
              " 'angeles',\n",
              " 'angelina',\n",
              " 'angelo',\n",
              " 'angelos',\n",
              " 'angels',\n",
              " 'angkor',\n",
              " 'anglia',\n",
              " 'angola',\n",
              " 'angresto',\n",
              " 'angulo',\n",
              " 'anh',\n",
              " 'anh_chàng',\n",
              " 'anh_chị',\n",
              " 'anh_chị_em',\n",
              " 'anh_dũng',\n",
              " 'anh_em',\n",
              " 'anh_em_trai',\n",
              " 'anh_hùng',\n",
              " 'anh_hùng_ca',\n",
              " 'anh_linh',\n",
              " 'anh_minh',\n",
              " 'anh_trai',\n",
              " 'anh_túc',\n",
              " 'anh_vũ',\n",
              " 'anh_ách',\n",
              " 'anh_ánh',\n",
              " 'anh_đào',\n",
              " 'anhto',\n",
              " 'anigo',\n",
              " 'anisensel',\n",
              " 'aniston',\n",
              " 'ankara',\n",
              " 'ann',\n",
              " 'anna',\n",
              " 'annan',\n",
              " 'anne',\n",
              " 'anolyte',\n",
              " 'another',\n",
              " 'anousone',\n",
              " 'ansar',\n",
              " 'antar',\n",
              " 'anten',\n",
              " 'anthony',\n",
              " 'anti',\n",
              " 'antigua',\n",
              " 'antioquia',\n",
              " 'antivirus',\n",
              " 'anto',\n",
              " 'antoine',\n",
              " 'antoinette',\n",
              " 'anton',\n",
              " 'antonio',\n",
              " 'antonioli',\n",
              " 'antonis',\n",
              " 'antony',\n",
              " 'antt',\n",
              " 'anwar',\n",
              " 'anz',\n",
              " 'anđt',\n",
              " 'ao',\n",
              " 'ao_ước',\n",
              " 'aol',\n",
              " 'ap',\n",
              " 'apache',\n",
              " 'apartheid',\n",
              " 'apd',\n",
              " 'apec',\n",
              " 'aperture',\n",
              " 'aphrodite',\n",
              " 'api',\n",
              " 'apollo',\n",
              " 'appf',\n",
              " 'appiah',\n",
              " 'apple',\n",
              " 'applet',\n",
              " 'application',\n",
              " 'apply',\n",
              " 'april',\n",
              " 'aps',\n",
              " 'apsara',\n",
              " 'aptech',\n",
              " 'apu',\n",
              " 'apwg',\n",
              " 'aqa',\n",
              " 'aqsa',\n",
              " 'aqua',\n",
              " 'aquarium',\n",
              " 'aquilani',\n",
              " 'aquino',\n",
              " 'ara',\n",
              " 'arab',\n",
              " 'araban',\n",
              " 'arabia',\n",
              " 'arabiya',\n",
              " 'arad',\n",
              " 'arafat',\n",
              " 'aragones',\n",
              " 'arakawa',\n",
              " 'aral',\n",
              " 'aranda',\n",
              " 'arango',\n",
              " 'aranzubia',\n",
              " 'araujo',\n",
              " 'arazi',\n",
              " 'archaeopteryx',\n",
              " 'architecture',\n",
              " 'archiver',\n",
              " 'archives',\n",
              " 'arctangza',\n",
              " 'are',\n",
              " 'arellano',\n",
              " 'arem',\n",
              " 'arena',\n",
              " 'argentina',\n",
              " 'argilli',\n",
              " 'arginine',\n",
              " 'argon',\n",
              " 'ari',\n",
              " 'aria',\n",
              " 'ariane',\n",
              " 'aric',\n",
              " 'arie',\n",
              " 'ariel',\n",
              " 'aris',\n",
              " 'aristide',\n",
              " 'ariza',\n",
              " 'arizona',\n",
              " 'arjen',\n",
              " 'arjhan',\n",
              " 'arkansas',\n",
              " 'arles',\n",
              " 'arlington',\n",
              " 'armand',\n",
              " 'armando',\n",
              " 'armani',\n",
              " 'armavia',\n",
              " 'armenia',\n",
              " 'arminia',\n",
              " 'armitage',\n",
              " 'armstrong',\n",
              " 'arn',\n",
              " 'arnaud',\n",
              " 'arne',\n",
              " 'arnold',\n",
              " 'arosa',\n",
              " 'array',\n",
              " 'arrese',\n",
              " 'arrichion',\n",
              " 'arrigo',\n",
              " 'arroyo',\n",
              " 'arruabarrena',\n",
              " 'arsen',\n",
              " 'arsenal',\n",
              " 'arsenal_chấp',\n",
              " 'arsene',\n",
              " 'arshavin',\n",
              " 'art',\n",
              " 'artemio',\n",
              " 'artemisinin',\n",
              " 'arteta',\n",
              " 'arthur',\n",
              " 'articleid',\n",
              " 'artmedia',\n",
              " 'arts',\n",
              " 'arturo',\n",
              " 'arv',\n",
              " 'arập',\n",
              " 'as',\n",
              " 'asa',\n",
              " 'asada',\n",
              " 'asahara',\n",
              " 'asahi',\n",
              " 'asamoah',\n",
              " 'ascender',\n",
              " 'ascii',\n",
              " 'ascoli',\n",
              " 'asda',\n",
              " 'asean',\n",
              " 'asefi',\n",
              " 'asem',\n",
              " 'asep',\n",
              " 'asf',\n",
              " 'asha',\n",
              " 'ashampoo',\n",
              " 'ashanti',\n",
              " 'ashcroft',\n",
              " 'ashdown',\n",
              " 'ashfaq',\n",
              " 'ashima',\n",
              " 'ashlee',\n",
              " 'ashley',\n",
              " 'ashraf',\n",
              " 'ashton',\n",
              " 'asia',\n",
              " 'asiad',\n",
              " 'asian',\n",
              " 'asianux',\n",
              " 'asic',\n",
              " 'asimo',\n",
              " 'asin',\n",
              " 'ask',\n",
              " 'aslan',\n",
              " 'aslem',\n",
              " 'asn',\n",
              " 'aso',\n",
              " 'asp',\n",
              " 'aspect',\n",
              " 'asperger',\n",
              " 'aspergillus',\n",
              " 'aspilia',\n",
              " 'aspire',\n",
              " 'aspirin',\n",
              " 'aspirine',\n",
              " 'aspx',\n",
              " 'assad',\n",
              " 'assam',\n",
              " 'assis',\n",
              " 'associates',\n",
              " 'association',\n",
              " 'astafjevs',\n",
              " 'astaman',\n",
              " 'aston',\n",
              " 'asuka',\n",
              " 'asus',\n",
              " 'at',\n",
              " 'ata',\n",
              " 'atak',\n",
              " 'atal',\n",
              " 'atalanta',\n",
              " 'atay',\n",
              " 'atb',\n",
              " 'atc',\n",
              " 'atf',\n",
              " 'atgt',\n",
              " 'athen',\n",
              " 'athens',\n",
              " 'athletic',\n",
              " 'athlon',\n",
              " 'ati',\n",
              " 'atisô',\n",
              " 'atl',\n",
              " 'atlanta',\n",
              " 'atlantic',\n",
              " 'atlantis',\n",
              " 'atlas',\n",
              " 'atletico',\n",
              " 'atm',\n",
              " 'atminfor',\n",
              " 'atomic',\n",
              " 'atp',\n",
              " 'atr',\n",
              " 'att',\n",
              " 'attack',\n",
              " 'atthipol',\n",
              " 'attila',\n",
              " 'attogiây',\n",
              " 'atvstp',\n",
              " 'atâu',\n",
              " 'au',\n",
              " 'auc',\n",
              " 'auckland',\n",
              " 'aud',\n",
              " 'audi',\n",
              " 'audio',\n",
              " 'audrey',\n",
              " 'augenthaler',\n",
              " 'august',\n",
              " 'augusto',\n",
              " 'augustus',\n",
              " 'aulas',\n",
              " 'aum',\n",
              " 'aung',\n",
              " 'aura',\n",
              " 'aurelio',\n",
              " 'austin',\n",
              " 'australia',\n",
              " 'australian',\n",
              " 'auto',\n",
              " 'autocad',\n",
              " 'autodesk',\n",
              " 'automatic',\n",
              " 'automobile',\n",
              " 'automotive',\n",
              " 'autopetro',\n",
              " 'auxerre',\n",
              " 'av',\n",
              " 'ava',\n",
              " 'avaya',\n",
              " 'avc',\n",
              " 'aventis',\n",
              " 'avenue',\n",
              " 'avery',\n",
              " 'avg',\n",
              " 'avi',\n",
              " 'aviator',\n",
              " 'aviv',\n",
              " 'avon',\n",
              " 'avramovic',\n",
              " 'avril',\n",
              " 'award',\n",
              " 'awards',\n",
              " 'aware',\n",
              " 'away',\n",
              " 'ax',\n",
              " 'axa',\n",
              " 'axel',\n",
              " 'axen',\n",
              " 'axit',\n",
              " 'axít',\n",
              " 'ay',\n",
              " 'ayad',\n",
              " 'ayala',\n",
              " 'aye',\n",
              " 'ayman',\n",
              " 'ayovi',\n",
              " 'ayre',\n",
              " 'ayrton',\n",
              " 'ayub',\n",
              " 'az',\n",
              " 'azahari',\n",
              " 'azerbaijan',\n",
              " 'azevedo',\n",
              " 'azithromycin',\n",
              " 'aziz',\n",
              " 'aztec',\n",
              " 'aztecs',\n",
              " 'azul',\n",
              " 'azzam',\n",
              " 'aê',\n",
              " 'ba',\n",
              " 'ba_ba',\n",
              " 'ba_bị',\n",
              " 'ba_bốn',\n",
              " 'ba_chỉ',\n",
              " 'ba_cùng',\n",
              " 'ba_gác',\n",
              " 'ba_hoa',\n",
              " 'ba_kích',\n",
              " 'ba_la',\n",
              " 'ba_lá',\n",
              " 'ba_lê',\n",
              " 'ba_lô',\n",
              " 'ba_má',\n",
              " 'ba_phải',\n",
              " 'ba_rọi',\n",
              " 'baath',\n",
              " 'babayaro',\n",
              " 'babel',\n",
              " 'babic',\n",
              " 'baby',\n",
              " 'babylon',\n",
              " 'bac',\n",
              " 'bach',\n",
              " 'bachelet',\n",
              " 'bachri',\n",
              " 'back',\n",
              " 'backdoor',\n",
              " 'backstedt',\n",
              " 'backstreet',\n",
              " 'backup',\n",
              " 'bacolod',\n",
              " 'bad',\n",
              " 'badawi',\n",
              " 'baden',\n",
              " 'bae',\n",
              " 'baez',\n",
              " 'baggio',\n",
              " 'baghdad',\n",
              " 'baghdatis',\n",
              " 'bagle',\n",
              " 'bahamas',\n",
              " 'bahnar',\n",
              " 'bahrain',\n",
              " 'baht',\n",
              " 'bai',\n",
              " 'baia',\n",
              " 'baiano',\n",
              " 'baidu',\n",
              " 'baihakki',\n",
              " 'baikal',\n",
              " 'baikonur',\n",
              " 'bakar',\n",
              " 'baker',\n",
              " 'bakery',\n",
              " 'bakiev',\n",
              " 'baku',\n",
              " 'baldini',\n",
              " 'baldwin',\n",
              " 'balears',\n",
              " 'bali',\n",
              " 'balkan',\n",
              " 'balkans',\n",
              " 'ball',\n",
              " 'ballack',\n",
              " 'ballad',\n",
              " 'balladur',\n",
              " 'ballast',\n",
              " 'ballesta',\n",
              " 'ballet',\n",
              " 'balli',\n",
              " 'ballmer',\n",
              " 'baltic',\n",
              " 'baltimore',\n",
              " 'balzaretti',\n",
              " 'balô',\n",
              " 'bam',\n",
              " 'bambang',\n",
              " 'ban',\n",
              " 'ban_bố',\n",
              " 'ban_công',\n",
              " 'ban_công_tác',\n",
              " 'ban_hành',\n",
              " 'ban_hành_pháp_lệnh',\n",
              " 'ban_hành_quyết_định',\n",
              " 'ban_hành_văn_bản',\n",
              " 'ban_mai',\n",
              " 'ban_ngành',\n",
              " 'ban_ngày',\n",
              " 'ban_phát',\n",
              " 'ban_tặng',\n",
              " 'ban_đêm',\n",
              " 'ban_đầu',\n",
              " 'ban_ơn',\n",
              " 'bana',\n",
              " 'band',\n",
              " 'banda',\n",
              " 'bandar',\n",
              " 'bandung',\n",
              " 'bang',\n",
              " 'bang_giao',\n",
              " 'bangalore',\n",
              " 'bangkok',\n",
              " 'bangladesh',\n",
              " 'banh',\n",
              " 'banh_nửa',\n",
              " 'bank',\n",
              " 'banking',\n",
              " 'banknet',\n",
              " 'banks',\n",
              " 'banluesak',\n",
              " 'banner',\n",
              " 'bao',\n",
              " 'bao_biện',\n",
              " 'bao_bì',\n",
              " 'bao_bọc',\n",
              " 'bao_che',\n",
              " 'bao_cấp',\n",
              " 'bao_dung',\n",
              " 'bao_giờ',\n",
              " 'bao_gói',\n",
              " 'bao_gồm',\n",
              " 'bao_hàm',\n",
              " 'bao_la',\n",
              " 'bao_lâu',\n",
              " 'bao_lâu_nay',\n",
              " 'bao_nhiêu',\n",
              " 'bao_phủ',\n",
              " 'bao_quát',\n",
              " 'bao_sân',\n",
              " 'bao_tay',\n",
              " 'bao_tiêu',\n",
              " 'bao_trùm',\n",
              " 'bao_tải',\n",
              " 'bao_tử',\n",
              " 'bao_vây',\n",
              " 'bao_xa',\n",
              " 'baptista',\n",
              " 'baquba',\n",
              " 'bar',\n",
              " 'baraja',\n",
              " 'barbara',\n",
              " 'barbie',\n",
              " 'barbiero',\n",
              " 'barbosa',\n",
              " 'barca',\n",
              " 'barcelona',\n",
              " 'barclays',\n",
              " 'bardot',\n",
              " 'barghouti',\n",
              " 'bari',\n",
              " 'baric',\n",
              " 'barie',\n",
              " 'barleygrass',\n",
              " 'barlow',\n",
              " 'barnes',\n",
              " 'barney',\n",
              " 'barnier',\n",
              " 'barone',\n",
              " 'baros',\n",
              " 'barr',\n",
              " 'barracuda',\n",
              " 'barreto',\n",
              " 'barrett',\n",
              " 'barrichello',\n",
              " 'barrie',\n",
              " 'barroso',\n",
              " 'barrot',\n",
              " 'barry',\n",
              " 'barrymore',\n",
              " 'bart',\n",
              " 'barthez',\n",
              " 'bartlett',\n",
              " 'barton',\n",
              " 'barwick',\n",
              " 'bas',\n",
              " 'basa',\n",
              " 'basayev',\n",
              " 'base',\n",
              " 'based',\n",
              " 'basedow',\n",
              " 'basel',\n",
              " 'bashir',\n",
              " 'basic',\n",
              " 'basil',\n",
              " 'basinas',\n",
              " 'basque',\n",
              " 'basra',\n",
              " 'bass',\n",
              " 'basso',\n",
              " 'basten',\n",
              " 'bastian',\n",
              " 'bat',\n",
              " 'bates',\n",
              " 'bath',\n",
              " 'baumann',\n",
              " 'baumgardner',\n",
              " 'baumgartner',\n",
              " 'bautista',\n",
              " 'bavaria',\n",
              " 'bavet',\n",
              " 'bay',\n",
              " 'bay_bướm',\n",
              " 'bay_bổng',\n",
              " 'bay_hơi',\n",
              " 'bay_lượn',\n",
              " 'bay_nhảy',\n",
              " 'bayati',\n",
              " 'bayer',\n",
              " 'bayern',\n",
              " 'bazan',\n",
              " 'bazzani',\n",
              " 'bađm',\n",
              " 'bb',\n",
              " 'bbc',\n",
              " 'bbt',\n",
              " 'bbàn',\n",
              " 'bc',\n",
              " 'bca',\n",
              " 'bcb',\n",
              " 'bcc',\n",
              " 'bch',\n",
              " 'bcht',\n",
              " 'bchtư',\n",
              " 'bci',\n",
              " 'bct',\n",
              " 'bcvt',\n",
              " 'bd',\n",
              " 'bdd',\n",
              " 'bdelloid',\n",
              " 'bdr',\n",
              " 'bdynamo',\n",
              " 'be',\n",
              " 'be_bé',\n",
              " 'be_bét',\n",
              " 'bea',\n",
              " 'beach',\n",
              " 'beagle',\n",
              " 'bean',\n",
              " 'beasley',\n",
              " 'beatles',\n",
              " 'beattie',\n",
              " 'beaujolais',\n",
              " 'beautiful',\n",
              " 'beauty',\n",
              " 'bec',\n",
              " 'becamex',\n",
              " 'becgiê',\n",
              " 'beck',\n",
              " 'beckenbauer',\n",
              " 'becker',\n",
              " 'beckham',\n",
              " 'becks',\n",
              " 'bee',\n",
              " 'been',\n",
              " 'beenhakker',\n",
              " 'beer',\n",
              " 'beethoven',\n",
              " 'bega',\n",
              " 'begin',\n",
              " 'behari',\n",
              " 'behr',\n",
              " 'beijing',\n",
              " 'beirut',\n",
              " 'bejan',\n",
              " 'belanov',\n",
              " 'belarus',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKWhjP5E5tfi"
      },
      "source": [
        "# ngram level - we choose max number of words equal to 30000 except all words (100k+ words)\n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', max_features=30000, ngram_range=(2, 3))\n",
        "tfidf_vect_ngram.fit(X_data)\n",
        "X_data_tfidf_ngram =  tfidf_vect_ngram.transform(X_data)\n",
        "# assume that we don't have test set before\n",
        "X_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "rorRgmE05tfi",
        "outputId": "c3356559-edeb-4ccf-8af6-beb5e3e01950",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tfidf_vect_ngram.get_feature_names()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['abu ghraib',\n",
              " 'ac milan',\n",
              " 'ac milan và',\n",
              " 'agribank cup',\n",
              " 'ai biết',\n",
              " 'ai có',\n",
              " 'ai có_thể',\n",
              " 'ai cũng',\n",
              " 'ai cũng biết',\n",
              " 'ai cũng có',\n",
              " 'ai cũng có_thể',\n",
              " 'ai cả',\n",
              " 'ai cập',\n",
              " 'ai dám',\n",
              " 'ai hết',\n",
              " 'ai khác',\n",
              " 'ai không',\n",
              " 'ai là',\n",
              " 'ai là người',\n",
              " 'ai làm',\n",
              " 'ai muốn',\n",
              " 'ai mà',\n",
              " 'ai nghĩ',\n",
              " 'ai nói',\n",
              " 'ai sẽ',\n",
              " 'ai trong',\n",
              " 'ai và',\n",
              " 'ai đã',\n",
              " 'ai đó',\n",
              " 'ai được',\n",
              " 'ajax amsterdam',\n",
              " 'al jazeera',\n",
              " 'al qaeda',\n",
              " 'al zarqawi',\n",
              " 'album của',\n",
              " 'album mới',\n",
              " 'album này',\n",
              " 'alex ferguson',\n",
              " 'alfred riedl',\n",
              " 'am subject',\n",
              " 'am subject gui',\n",
              " 'an bình',\n",
              " 'an cho',\n",
              " 'an giang',\n",
              " 'an phú',\n",
              " 'an và',\n",
              " 'an đã',\n",
              " 'an_ninh cho',\n",
              " 'an_ninh của',\n",
              " 'an_ninh mạng',\n",
              " 'an_ninh quốc_gia',\n",
              " 'an_ninh trật_tự',\n",
              " 'an_ninh và',\n",
              " 'an_ninh điều_tra',\n",
              " 'an_ninh điều_tra bộ',\n",
              " 'an_toàn cho',\n",
              " 'an_toàn của',\n",
              " 'an_toàn giao_thông',\n",
              " 'an_toàn hơn',\n",
              " 'an_toàn thực_phẩm',\n",
              " 'an_toàn trong',\n",
              " 'an_toàn và',\n",
              " 'andre agassi',\n",
              " 'andrew carnegie',\n",
              " 'andy roddick',\n",
              " 'anh anh',\n",
              " 'anh biết',\n",
              " 'anh bình',\n",
              " 'anh bạn',\n",
              " 'anh bảo',\n",
              " 'anh bằng',\n",
              " 'anh bị',\n",
              " 'anh cho',\n",
              " 'anh cho biết',\n",
              " 'anh cho rằng',\n",
              " 'anh chính',\n",
              " 'anh chưa',\n",
              " 'anh chẳng',\n",
              " 'anh chỉ',\n",
              " 'anh chồng',\n",
              " 'anh các',\n",
              " 'anh còn',\n",
              " 'anh có',\n",
              " 'anh có_thể',\n",
              " 'anh cô',\n",
              " 'anh cùng',\n",
              " 'anh cũng',\n",
              " 'anh cũng không',\n",
              " 'anh cũng đã',\n",
              " 'anh cảm_thấy',\n",
              " 'anh cần',\n",
              " 'anh của',\n",
              " 'anh cứ',\n",
              " 'anh dành',\n",
              " 'anh gặp',\n",
              " 'anh hay',\n",
              " 'anh hiểu',\n",
              " 'anh hoàng',\n",
              " 'anh hãy',\n",
              " 'anh hơn',\n",
              " 'anh hải',\n",
              " 'anh họ',\n",
              " 'anh khi',\n",
              " 'anh khoa',\n",
              " 'anh khánh',\n",
              " 'anh không',\n",
              " 'anh không có',\n",
              " 'anh không phải',\n",
              " 'anh không_thể',\n",
              " 'anh kể',\n",
              " 'anh long',\n",
              " 'anh luôn',\n",
              " 'anh là',\n",
              " 'anh là một',\n",
              " 'anh là người',\n",
              " 'anh làm',\n",
              " 'anh lâm',\n",
              " 'anh lê',\n",
              " 'anh lại',\n",
              " 'anh lộc',\n",
              " 'anh muốn',\n",
              " 'anh mà',\n",
              " 'anh một',\n",
              " 'anh mới',\n",
              " 'anh mỹ',\n",
              " 'anh nghĩ',\n",
              " 'anh nghĩ sao',\n",
              " 'anh nguyễn',\n",
              " 'anh nguyễn văn',\n",
              " 'anh người',\n",
              " 'anh ngữ',\n",
              " 'anh nhiều',\n",
              " 'anh như',\n",
              " 'anh nhưng',\n",
              " 'anh nhận',\n",
              " 'anh những',\n",
              " 'anh này',\n",
              " 'anh nên',\n",
              " 'anh nói',\n",
              " 'anh năm',\n",
              " 'anh nếu',\n",
              " 'anh nữa',\n",
              " 'anh phan',\n",
              " 'anh pháp',\n",
              " 'anh phạm',\n",
              " 'anh phải',\n",
              " 'anh quang',\n",
              " 'anh quá',\n",
              " 'anh quốc',\n",
              " 'anh ra',\n",
              " 'anh rất',\n",
              " 'anh rằng',\n",
              " 'anh sau',\n",
              " 'anh sau tt',\n",
              " 'anh sơn',\n",
              " 'anh sẽ',\n",
              " 'anh ta',\n",
              " 'anh ta có',\n",
              " 'anh ta cũng',\n",
              " 'anh ta không',\n",
              " 'anh ta là',\n",
              " 'anh ta sẽ',\n",
              " 'anh ta đã',\n",
              " 'anh theo',\n",
              " 'anh thành',\n",
              " 'anh thì',\n",
              " 'anh thư',\n",
              " 'anh thường',\n",
              " 'anh thấy',\n",
              " 'anh thật',\n",
              " 'anh thắng',\n",
              " 'anh tiếp_tục',\n",
              " 'anh tony',\n",
              " 'anh tony blair',\n",
              " 'anh trong',\n",
              " 'anh trung',\n",
              " 'anh trên',\n",
              " 'anh trí',\n",
              " 'anh trước',\n",
              " 'anh trần',\n",
              " 'anh tuấn',\n",
              " 'anh tuổi',\n",
              " 'anh tài',\n",
              " 'anh tâm',\n",
              " 'anh tâm_sự',\n",
              " 'anh tôi',\n",
              " 'anh tú',\n",
              " 'anh tại',\n",
              " 'anh từ',\n",
              " 'anh từng',\n",
              " 'anh tự',\n",
              " 'anh viết',\n",
              " 'anh việt',\n",
              " 'anh và',\n",
              " 'anh vào',\n",
              " 'anh vì',\n",
              " 'anh văn',\n",
              " 'anh vẫn',\n",
              " 'anh vẫn còn',\n",
              " 'anh về',\n",
              " 'anh với',\n",
              " 'anh vừa',\n",
              " 'anh yêu',\n",
              " 'anh đang',\n",
              " 'anh đi',\n",
              " 'anh đã',\n",
              " 'anh đã bị',\n",
              " 'anh đã có',\n",
              " 'anh đã không',\n",
              " 'anh đã làm',\n",
              " 'anh đã được',\n",
              " 'anh đó',\n",
              " 'anh đưa',\n",
              " 'anh được',\n",
              " 'anh đến',\n",
              " 'anh đều',\n",
              " 'anh để',\n",
              " 'anh đức',\n",
              " 'anh đừng',\n",
              " 'anh ấy',\n",
              " 'anh ấy biết',\n",
              " 'anh ấy còn',\n",
              " 'anh ấy có',\n",
              " 'anh ấy cũng',\n",
              " 'anh ấy không',\n",
              " 'anh ấy là',\n",
              " 'anh ấy lại',\n",
              " 'anh ấy nói',\n",
              " 'anh ấy rất',\n",
              " 'anh ấy sẽ',\n",
              " 'anh ấy tôi',\n",
              " 'anh ấy và',\n",
              " 'anh ấy vẫn',\n",
              " 'anh ấy đã',\n",
              " 'anh_em trong',\n",
              " 'ariel sharon',\n",
              " 'arsenal chelsea',\n",
              " 'arsenal cũng',\n",
              " 'arsenal sẽ',\n",
              " 'arsenal và',\n",
              " 'arsenal vẫn',\n",
              " 'arsenal đã',\n",
              " 'as roma',\n",
              " 'ashley cole',\n",
              " 'asian cup',\n",
              " 'aston villa',\n",
              " 'athletic bilbao',\n",
              " 'atletico madrid',\n",
              " 'australia mở_rộng',\n",
              " 'australia và',\n",
              " 'axit béo',\n",
              " 'ba bàn',\n",
              " 'ba của',\n",
              " 'ba lan',\n",
              " 'ba là',\n",
              " 'ba lần',\n",
              " 'ba mẹ',\n",
              " 'ba ngày',\n",
              " 'ba người',\n",
              " 'ba năm',\n",
              " 'ba tháng',\n",
              " 'ba trong',\n",
              " 'ba trận',\n",
              " 'ba tôi',\n",
              " 'ba và',\n",
              " 'ba điểm',\n",
              " 'ba đình',\n",
              " 'ban bí_thư',\n",
              " 'ban chuyên_án',\n",
              " 'ban châu',\n",
              " 'ban châu âu',\n",
              " 'ban chấp_hành',\n",
              " 'ban chấp_hành trung_ương',\n",
              " 'ban chỉ_huy',\n",
              " 'ban chỉ_đạo',\n",
              " 'ban cán_sự',\n",
              " 'ban giám_khảo',\n",
              " 'ban giám_đốc',\n",
              " 'ban huấn_luyện',\n",
              " 'ban kinh_tế',\n",
              " 'ban kỷ_luật',\n",
              " 'ban lãnh_đạo',\n",
              " 'ban nhạc',\n",
              " 'ban quản_lý',\n",
              " 'ban quản_lý chợ',\n",
              " 'ban quản_lý các',\n",
              " 'ban quản_lý dự_án',\n",
              " 'ban tổ_chức',\n",
              " 'ban tổ_chức đã',\n",
              " 'ban văn',\n",
              " 'ban văn hoátiêu',\n",
              " 'ban điều_hành',\n",
              " 'ban_đầu cho',\n",
              " 'ban_đầu của',\n",
              " 'ban_đầu là',\n",
              " 'bao cao_su',\n",
              " 'bao năm',\n",
              " 'bao quanh',\n",
              " 'bao_giờ có',\n",
              " 'bao_giờ cũng',\n",
              " 'bao_giờ hết',\n",
              " 'bao_giờ là',\n",
              " 'bao_giờ nghĩ',\n",
              " 'bao_giờ quên',\n",
              " 'bao_giờ tôi',\n",
              " 'bao_giờ được',\n",
              " 'bao_gồm các',\n",
              " 'bao_gồm cả',\n",
              " 'bao_gồm một',\n",
              " 'bao_nhiêu người',\n",
              " 'bao_nhiêu năm',\n",
              " 'bao_nhiêu thì',\n",
              " 'bao_nhiêu tiền',\n",
              " 'barcelona và',\n",
              " 'barcelona đã',\n",
              " 'bay của',\n",
              " 'bay lên',\n",
              " 'bay người',\n",
              " 'bay sang',\n",
              " 'bay tới',\n",
              " 'bay từ',\n",
              " 'bay vào',\n",
              " 'bay về',\n",
              " 'bayern munich',\n",
              " 'bbàn thắng',\n",
              " 'bbàn thắng bàn',\n",
              " 'bch khóa',\n",
              " 'bia rượu',\n",
              " 'bill clinton',\n",
              " 'bill gates',\n",
              " 'bin laden',\n",
              " 'binh_sĩ mỹ',\n",
              " 'biên hòa',\n",
              " 'biên phủ',\n",
              " 'biến mất',\n",
              " 'biến thành',\n",
              " 'biến thành một',\n",
              " 'biết anh',\n",
              " 'biết bà',\n",
              " 'biết bạn',\n",
              " 'biết bộ',\n",
              " 'biết chiếc',\n",
              " 'biết chuyện',\n",
              " 'biết chính_xác',\n",
              " 'biết chúng_tôi',\n",
              " 'biết chắc',\n",
              " 'biết chỉ',\n",
              " 'biết chị',\n",
              " 'biết con',\n",
              " 'biết các',\n",
              " 'biết cách',\n",
              " 'biết có',\n",
              " 'biết có_thể',\n",
              " 'biết cô',\n",
              " 'biết công_ty',\n",
              " 'biết cũng',\n",
              " 'biết cơ_quan',\n",
              " 'biết cả',\n",
              " 'biết do',\n",
              " 'biết em',\n",
              " 'biết giá',\n",
              " 'biết gì',\n",
              " 'biết gì về',\n",
              " 'biết hai',\n",
              " 'biết hiện',\n",
              " 'biết hiện_nay',\n",
              " 'biết họ',\n",
              " 'biết họ đã',\n",
              " 'biết khi',\n",
              " 'biết khoảng',\n",
              " 'biết không',\n",
              " 'biết kết_quả',\n",
              " 'biết là',\n",
              " 'biết làm',\n",
              " 'biết mình',\n",
              " 'biết mỗi',\n",
              " 'biết một',\n",
              " 'biết một_số',\n",
              " 'biết ngay',\n",
              " 'biết ngoài',\n",
              " 'biết nguyên_nhân',\n",
              " 'biết ngày',\n",
              " 'biết người',\n",
              " 'biết nhiều',\n",
              " 'biết như',\n",
              " 'biết nhưng',\n",
              " 'biết những',\n",
              " 'biết nên',\n",
              " 'biết nó',\n",
              " 'biết nói',\n",
              " 'biết năm',\n",
              " 'biết nếu',\n",
              " 'biết phải',\n",
              " 'biết qua',\n",
              " 'biết rõ',\n",
              " 'biết rất',\n",
              " 'biết rằng',\n",
              " 'biết sau',\n",
              " 'biết sau khi',\n",
              " 'biết sẽ',\n",
              " 'biết số',\n",
              " 'biết sự',\n",
              " 'biết theo',\n",
              " 'biết thêm',\n",
              " 'biết thì',\n",
              " 'biết thông_tin',\n",
              " 'biết tin',\n",
              " 'biết tiếng',\n",
              " 'biết trong',\n",
              " 'biết trước',\n",
              " 'biết tôi',\n",
              " 'biết tại',\n",
              " 'biết tất_cả',\n",
              " 'biết tới',\n",
              " 'biết từ',\n",
              " 'biết tự',\n",
              " 'biết việc',\n",
              " 'biết và',\n",
              " 'biết vào',\n",
              " 'biết vì',\n",
              " 'biết về',\n",
              " 'biết với',\n",
              " 'biết vụ',\n",
              " 'biết ông',\n",
              " 'biết đang',\n",
              " 'biết điều',\n",
              " 'biết đây',\n",
              " 'biết đây là',\n",
              " 'biết đã',\n",
              " 'biết đó',\n",
              " 'biết đó là',\n",
              " 'biết được',\n",
              " 'biết đến',\n",
              " 'biết đến nay',\n",
              " 'biết đến với',\n",
              " 'biết để',\n",
              " 'biển ngà',\n",
              " 'biển phía',\n",
              " 'biển số',\n",
              " 'biển và',\n",
              " 'biển đã',\n",
              " 'biển đông',\n",
              " 'biểu_diễn của',\n",
              " 'biểu_diễn nghệ_thuật',\n",
              " 'biểu_diễn tại',\n",
              " 'biểu_hiện của',\n",
              " 'biểu_tình phản_đối',\n",
              " 'biểu_tượng của',\n",
              " 'biện_pháp nhằm',\n",
              " 'biện_pháp này',\n",
              " 'biện_pháp xử_lý',\n",
              " 'biện_pháp để',\n",
              " 'blu ray',\n",
              " 'bom tự_sát',\n",
              " 'br vt',\n",
              " 'brazil này',\n",
              " 'brazil và',\n",
              " 'brazil đã',\n",
              " 'britney spears',\n",
              " 'btc giải',\n",
              " 'bush và',\n",
              " 'bush đã',\n",
              " 'buôn_lậu xăng',\n",
              " 'buôn_lậu xăng dầu',\n",
              " 'buồn của',\n",
              " 'buồn khi',\n",
              " 'buồn lắm',\n",
              " 'buồn và',\n",
              " 'buồn vì',\n",
              " 'buổi biểu_diễn',\n",
              " 'buổi chiều',\n",
              " 'buổi họp',\n",
              " 'buổi họp_báo',\n",
              " 'buổi hội_thảo',\n",
              " 'buổi làm_việc',\n",
              " 'buổi làm_việc với',\n",
              " 'buổi lễ',\n",
              " 'buổi sáng',\n",
              " 'buổi tiệc',\n",
              " 'buổi trưa',\n",
              " 'buổi tư_vấn',\n",
              " 'buổi tập',\n",
              " 'buổi tối',\n",
              " 'buộc các',\n",
              " 'buộc họ',\n",
              " 'buộc phải',\n",
              " 'buộc phải thắng',\n",
              " 'bv nhi_đồng',\n",
              " 'bà arroyo',\n",
              " 'bà cho',\n",
              " 'bà chủ',\n",
              " 'bà có',\n",
              " 'bà cũng',\n",
              " 'bà cụ',\n",
              " 'bà hoa',\n",
              " 'bà hà',\n",
              " 'bà không',\n",
              " 'bà là',\n",
              " 'bà lê',\n",
              " 'bà lê thị',\n",
              " 'bà mẹ',\n",
              " 'bà ngoại',\n",
              " 'bà nguyễn',\n",
              " 'bà nguyễn thị',\n",
              " 'bà nói',\n",
              " 'bà nội',\n",
              " 'bà phạm',\n",
              " 'bà rịa',\n",
              " 'bà rịa vũng_tàu',\n",
              " 'bà sẽ',\n",
              " 'bà ta',\n",
              " 'bà triệu',\n",
              " 'bà trưng',\n",
              " 'bà trưng hà_nội',\n",
              " 'bà trần',\n",
              " 'bà trần thị',\n",
              " 'bà và',\n",
              " 'bà vợ',\n",
              " 'bà đã',\n",
              " 'bà đầm',\n",
              " 'bà đầm già',\n",
              " 'bà ấy',\n",
              " 'bài báo',\n",
              " 'bài ca',\n",
              " 'bài của',\n",
              " 'bài hát',\n",
              " 'bài hát của',\n",
              " 'bài kiểm_tra',\n",
              " 'bài này',\n",
              " 'bài phát_biểu',\n",
              " 'bài thi',\n",
              " 'bài thuốc',\n",
              " 'bài thơ',\n",
              " 'bài viết',\n",
              " 'bài viết của',\n",
              " 'bàn cho',\n",
              " 'bàn các',\n",
              " 'bàn của',\n",
              " 'bàn ghế',\n",
              " 'bàn gỡ',\n",
              " 'bàn hàng_đầu',\n",
              " 'bàn không',\n",
              " 'bàn mở',\n",
              " 'bàn mở tỷ_số',\n",
              " 'bàn nhưng',\n",
              " 'bàn phá',\n",
              " 'bàn phá thế',\n",
              " 'bàn thua',\n",
              " 'bàn thua điểm',\n",
              " 'bàn thắng',\n",
              " 'bàn thắng cho',\n",
              " 'bàn thắng của',\n",
              " 'bàn thắng duy_nhất',\n",
              " 'bàn thắng mở',\n",
              " 'bàn thắng nào',\n",
              " 'bàn thắng này',\n",
              " 'bàn thắng phút',\n",
              " 'bàn thắng thứ',\n",
              " 'bàn thắng trong',\n",
              " 'bàn thắng trên',\n",
              " 'bàn thắng và',\n",
              " 'bàn thắng đầu_tiên',\n",
              " 'bàn thắng_bại',\n",
              " 'bàn trong',\n",
              " 'bàn và',\n",
              " 'bàn về',\n",
              " 'bàn với',\n",
              " 'bàn ăn',\n",
              " 'bàn đến',\n",
              " 'bàn_bạc với',\n",
              " 'bàn_tay của',\n",
              " 'bào_chữa cho',\n",
              " 'bày bán',\n",
              " 'bày_tỏ sự',\n",
              " 'bác hồ',\n",
              " 'bác đơn',\n",
              " 'bác_sĩ cho',\n",
              " 'bác_sĩ cho biết',\n",
              " 'bác_sĩ chuyên_khoa',\n",
              " 'bác_sĩ của',\n",
              " 'bác_sĩ lê',\n",
              " 'bác_sĩ nguyễn',\n",
              " 'bác_sĩ sẽ',\n",
              " 'bác_sĩ trần',\n",
              " 'bác_sĩ và',\n",
              " 'bác_sĩ đã',\n",
              " 'bách_khoa hà_nội',\n",
              " 'bám sát',\n",
              " 'bám theo',\n",
              " 'bám vào',\n",
              " 'bám đuổi',\n",
              " 'bán cho',\n",
              " 'bán các',\n",
              " 'bán hàng',\n",
              " 'bán hết',\n",
              " 'bán lại',\n",
              " 'bán lại cho',\n",
              " 'bán nhà',\n",
              " 'bán những',\n",
              " 'bán nền',\n",
              " 'bán phá_giá',\n",
              " 'bán phá_giá tôm',\n",
              " 'bán ra',\n",
              " 'bán thuốc',\n",
              " 'bán trên',\n",
              " 'bán tại',\n",
              " 'bán vé',\n",
              " 'bán với',\n",
              " 'bán với giá',\n",
              " 'bán được',\n",
              " 'bán đất',\n",
              " 'bán đấu_giá',\n",
              " 'bán_chạy nhất',\n",
              " 'bán_kết champions',\n",
              " 'bán_kết champions league',\n",
              " 'bán_kết lượt',\n",
              " 'bán_kết và',\n",
              " 'bán_độ của',\n",
              " 'bánh heroin',\n",
              " 'bánh kẹo',\n",
              " 'báo cho',\n",
              " 'báo có',\n",
              " 'báo của',\n",
              " 'báo giới',\n",
              " 'báo người',\n",
              " 'báo người lao_động',\n",
              " 'báo nlđ',\n",
              " 'báo này',\n",
              " 'báo thanh_niên',\n",
              " 'báo thanh_niên đã',\n",
              " 'báo tin',\n",
              " 'báo trước',\n",
              " 'báo tuổi_trẻ',\n",
              " 'báo và',\n",
              " 'báo về',\n",
              " 'báo với',\n",
              " 'báo điện_tử',\n",
              " 'báo đài',\n",
              " 'báo đã',\n",
              " 'báo_chí anh',\n",
              " 'báo_chí và',\n",
              " 'báo_chí đã',\n",
              " 'báo_cáo của',\n",
              " 'báo_cáo kết_quả',\n",
              " 'báo_cáo lên',\n",
              " 'báo_cáo về',\n",
              " 'báo_cáo với',\n",
              " 'bây_giờ anh',\n",
              " 'bây_giờ có',\n",
              " 'bây_giờ không',\n",
              " 'bây_giờ là',\n",
              " 'bây_giờ thì',\n",
              " 'bây_giờ tôi',\n",
              " 'bây_giờ đã',\n",
              " 'bãi biển',\n",
              " 'bãi cháy',\n",
              " 'bãi rác',\n",
              " 'bão số',\n",
              " 'bé bị',\n",
              " 'bé có',\n",
              " 'bé gái',\n",
              " 'bé không',\n",
              " 'bé sẽ',\n",
              " 'bé trai',\n",
              " 'bé trong',\n",
              " 'bé tuổi',\n",
              " 'bé và',\n",
              " 'bé đã',\n",
              " 'béo phì',\n",
              " 'bên anh',\n",
              " 'bên bờ',\n",
              " 'bên cánh',\n",
              " 'bên cánh phải',\n",
              " 'bên cánh trái',\n",
              " 'bên cạnh',\n",
              " 'bên cạnh các',\n",
              " 'bên cạnh những',\n",
              " 'bên cạnh việc',\n",
              " 'bên cạnh đó',\n",
              " 'bên dưới',\n",
              " 'bên hành_lang',\n",
              " 'bên hông',\n",
              " 'bên kia',\n",
              " 'bên là',\n",
              " 'bên lề',\n",
              " 'bên ngoài',\n",
              " 'bên người',\n",
              " 'bên nhau',\n",
              " 'bên này',\n",
              " 'bên phía',\n",
              " 'bên phải',\n",
              " 'bên sẽ',\n",
              " 'bên trong',\n",
              " 'bên trái',\n",
              " 'bên trên',\n",
              " 'bên đã',\n",
              " 'bên đường',\n",
              " 'bên đều',\n",
              " 'bình an',\n",
              " 'bình cho',\n",
              " 'bình chánh',\n",
              " 'bình dương',\n",
              " 'bình dương bình',\n",
              " 'bình dương và',\n",
              " 'bình dương đã',\n",
              " 'bình hưng',\n",
              " 'bình nhưỡng',\n",
              " 'bình phước',\n",
              " 'bình quới',\n",
              " 'bình thuận',\n",
              " 'bình thạnh',\n",
              " 'bình thạnh tp',\n",
              " 'bình tp',\n",
              " 'bình tp hcm',\n",
              " 'bình triệu',\n",
              " 'bình tân',\n",
              " 'bình và',\n",
              " 'bình đã',\n",
              " 'bình định',\n",
              " 'bình_luận về',\n",
              " 'bình_phục chấn_thương',\n",
              " 'bình_quân mỗi',\n",
              " 'bình_thuận đã',\n",
              " 'bình_thường của',\n",
              " 'bình_thường không',\n",
              " 'bình_thường như',\n",
              " 'bình_thường nhưng',\n",
              " 'bình_thường trong',\n",
              " 'bình_thường và',\n",
              " 'bình_tĩnh và',\n",
              " 'bí_mật của',\n",
              " 'bí_thư thành_ủy',\n",
              " 'bí_thư tỉnh_uỷ',\n",
              " 'bính tuất',\n",
              " 'bó hoa',\n",
              " 'bó_tay thủ_môn',\n",
              " 'bóc vỏ',\n",
              " 'bóng anh',\n",
              " 'bóng bên',\n",
              " 'bóng bên cánh',\n",
              " 'bóng bật',\n",
              " 'bóng bằng',\n",
              " 'bóng bổng',\n",
              " 'bóng cho',\n",
              " 'bóng chạm',\n",
              " 'bóng có',\n",
              " 'bóng cùng',\n",
              " 'bóng cũ',\n",
              " 'bóng của',\n",
              " 'bóng của hlv',\n",
              " 'bóng của mình',\n",
              " 'bóng của ông',\n",
              " 'bóng hoàng_gia',\n",
              " 'bóng không',\n",
              " 'bóng lại',\n",
              " 'bóng lớn',\n",
              " 'bóng mà',\n",
              " 'bóng mạnh',\n",
              " 'bóng mới',\n",
              " 'bóng ngay',\n",
              " 'bóng nguy_hiểm',\n",
              " 'bóng nhiều',\n",
              " 'bóng nhiều hơn',\n",
              " 'bóng nhưng',\n",
              " 'bóng nào',\n",
              " 'bóng này',\n",
              " 'bóng năm',\n",
              " 'bóng năm nay',\n",
              " 'bóng phố',\n",
              " 'bóng phố núi',\n",
              " 'bóng qua',\n",
              " 'bóng quân_đội',\n",
              " 'bóng ra',\n",
              " 'bóng rất',\n",
              " 'bóng sau',\n",
              " 'bóng sông',\n",
              " 'bóng thành',\n",
              " 'bóng thành london',\n",
              " 'bóng thành_phố',\n",
              " 'bóng thành_phố cảng',\n",
              " 'bóng thủ_đô',\n",
              " 'bóng trong',\n",
              " 'bóng trước',\n",
              " 'bóng tại',\n",
              " 'bóng tốt',\n",
              " 'bóng tới',\n",
              " 'bóng từ',\n",
              " 'bóng và',\n",
              " 'bóng vàng',\n",
              " 'bóng vàng châu',\n",
              " 'bóng vào',\n",
              " 'bóng vào góc',\n",
              " 'bóng vào lưới',\n",
              " 'bóng với',\n",
              " 'bóng xứ',\n",
              " 'bóng xứ catalan',\n",
              " 'bóng xứ nghệ',\n",
              " 'bóng áo',\n",
              " 'bóng đang',\n",
              " 'bóng đi',\n",
              " 'bóng đi chệch',\n",
              " 'bóng đã',\n",
              " 'bóng đèn',\n",
              " 'bóng được',\n",
              " 'bóng đầu_tiên',\n",
              " 'bóng đến',\n",
              " 'bóng đến từ',\n",
              " 'bóng để',\n",
              " 'bóng_đá anh',\n",
              " 'bóng_đá châu',\n",
              " 'bóng_đá châu âu',\n",
              " 'bóng_đá của',\n",
              " 'bóng_đá nam',\n",
              " 'bóng_đá nước_nhà',\n",
              " 'bóng_đá nữ',\n",
              " 'bóng_đá quốc_tế',\n",
              " 'bóng_đá thế_giới',\n",
              " 'bóng_đá tp',\n",
              " 'bóng_đá tp hcm',\n",
              " 'bóng_đá trong',\n",
              " 'bóng_đá trẻ',\n",
              " 'bóng_đá việt',\n",
              " 'bóng_đá việt nam',\n",
              " 'bóng_đá việt_nam',\n",
              " 'bóng_đá vn',\n",
              " 'bóng_đá và',\n",
              " 'bông hoa',\n",
              " 'bông sen',\n",
              " 'bù giờ',\n",
              " 'bù lại',\n",
              " 'bùi ngọc',\n",
              " 'bùi quang',\n",
              " 'bùi quang hưng',\n",
              " 'bùi thị',\n",
              " 'bùi thị nhung',\n",
              " 'bùi tiến',\n",
              " 'bùi tiến dũng',\n",
              " 'bùi văn',\n",
              " 'bùng lên',\n",
              " 'bú mẹ',\n",
              " 'băm nhuyễn',\n",
              " 'băm nhỏ',\n",
              " 'băng cướp',\n",
              " 'băng ghế',\n",
              " 'băng ghế dự_bị',\n",
              " 'băng lên',\n",
              " 'băng qua',\n",
              " 'băng vào',\n",
              " 'băng xuống',\n",
              " 'băng đĩa',\n",
              " 'bơm nước',\n",
              " 'bưu_chính viễn_thông',\n",
              " 'bưu_điện hà_nội',\n",
              " 'bưu_điện mỹ',\n",
              " 'bước chân',\n",
              " 'bước chân vào',\n",
              " 'bước khởi_đầu',\n",
              " 'bước lên',\n",
              " 'bước qua',\n",
              " 'bước ra',\n",
              " 'bước sang',\n",
              " 'bước sang tuổi',\n",
              " 'bước tiếp_theo',\n",
              " 'bước vào',\n",
              " 'bước vào trận',\n",
              " 'bước xuống',\n",
              " 'bước đột_phá',\n",
              " 'bạc liêu',\n",
              " 'bạch mai',\n",
              " 'bạch đằng',\n",
              " 'bạn anh',\n",
              " 'bạn biết',\n",
              " 'bạn bạn',\n",
              " 'bạn bị',\n",
              " 'bạn cho',\n",
              " 'bạn chưa',\n",
              " 'bạn chỉ',\n",
              " 'bạn chỉ cần',\n",
              " 'bạn chọn',\n",
              " 'bạn còn',\n",
              " 'bạn có',\n",
              " 'bạn có_thể',\n",
              " 'bạn cùng',\n",
              " 'bạn cũng',\n",
              " 'bạn cũng có_thể',\n",
              " 'bạn cũng_nên',\n",
              " 'bạn cảm_thấy',\n",
              " 'bạn cần',\n",
              " 'bạn cần phải',\n",
              " 'bạn của',\n",
              " 'bạn cứ',\n",
              " 'bạn diễn',\n",
              " 'bạn dùng',\n",
              " 'bạn gái',\n",
              " 'bạn gái anh',\n",
              " 'bạn gái của',\n",
              " 'bạn gái tôi',\n",
              " 'bạn hãy',\n",
              " 'bạn học',\n",
              " 'bạn khi',\n",
              " 'bạn không',\n",
              " 'bạn không nên',\n",
              " 'bạn không_thể',\n",
              " 'bạn luôn',\n",
              " 'bạn là',\n",
              " 'bạn là một',\n",
              " 'bạn là người',\n",
              " 'bạn làm',\n",
              " 'bạn lại',\n",
              " 'bạn muốn',\n",
              " 'bạn mà',\n",
              " 'bạn mình',\n",
              " 'bạn một',\n",
              " 'bạn mới',\n",
              " 'bạn nghĩ',\n",
              " 'bạn người',\n",
              " 'bạn như',\n",
              " 'bạn nhưng',\n",
              " 'bạn những',\n",
              " 'bạn nên',\n",
              " 'bạn nói',\n",
              " 'bạn nếu',\n",
              " 'bạn phải',\n",
              " 'bạn rất',\n",
              " 'bạn sẽ',\n",
              " 'bạn sẽ có',\n",
              " 'bạn sẽ không',\n",
              " 'bạn sẽ phải',\n",
              " 'bạn sẽ thấy',\n",
              " 'bạn sẽ được',\n",
              " 'bạn thân',\n",
              " 'bạn thân của',\n",
              " 'bạn thì',\n",
              " 'bạn thích',\n",
              " 'bạn thường',\n",
              " 'bạn thấy',\n",
              " 'bạn trai',\n",
              " 'bạn trai của',\n",
              " 'bạn trong',\n",
              " 'bạn trẻ',\n",
              " 'bạn tình',\n",
              " 'bạn tôi',\n",
              " 'bạn tốt',\n",
              " 'bạn và',\n",
              " 'bạn vào',\n",
              " 'bạn vì',\n",
              " 'bạn vẫn',\n",
              " 'bạn về',\n",
              " 'bạn với',\n",
              " 'bạn yêu',\n",
              " 'bạn đang',\n",
              " 'bạn đi',\n",
              " 'bạn đã',\n",
              " 'bạn đã có',\n",
              " 'bạn được',\n",
              " 'bạn đến',\n",
              " 'bạn để',\n",
              " 'bạn đừng',\n",
              " 'bạn_bè của',\n",
              " 'bạn_bè và',\n",
              " 'bạn_đời của',\n",
              " 'bản báo_cáo',\n",
              " 'bản dùng',\n",
              " 'bản dùng thử',\n",
              " 'bản dự_thảo',\n",
              " 'bản hợp_đồng',\n",
              " 'bản sẽ',\n",
              " 'bản tin',\n",
              " 'bản tường_trình',\n",
              " 'bản và',\n",
              " 'bản vá',\n",
              " 'bản đã',\n",
              " 'bản_chất của',\n",
              " 'bản_lĩnh của',\n",
              " 'bản_lĩnh và',\n",
              " 'bản_thân anh',\n",
              " 'bản_thân mình',\n",
              " 'bản_thân tôi',\n",
              " 'bản_thân tôi cũng',\n",
              " 'bản_thân và',\n",
              " 'bản_thân ông',\n",
              " 'bản_án sơ_thẩm',\n",
              " 'bảng anh',\n",
              " 'bảng champions',\n",
              " 'bảng champions league',\n",
              " 'bảng sau',\n",
              " 'bảng trận',\n",
              " 'bảng trận điểm',\n",
              " 'bảng tổng_sắp',\n",
              " 'bảng và',\n",
              " 'bảng với',\n",
              " 'bảng xếp_hạng',\n",
              " 'bảng đấu',\n",
              " 'bảng đội',\n",
              " 'bảo anh',\n",
              " 'bảo khanh',\n",
              " 'bảo là',\n",
              " 'bảo quốc',\n",
              " 'bảo rằng',\n",
              " 'bảo tôi',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWk8av_U5tfj"
      },
      "source": [
        "# ngram-char level - we choose max number of words equal to 30000 except all words (100k+ words)\n",
        "tfidf_vect_ngram_char = TfidfVectorizer(analyzer='char', max_features=30000, ngram_range=(2, 3))\n",
        "tfidf_vect_ngram_char.fit(X_data)\n",
        "X_data_tfidf_ngram_char =  tfidf_vect_ngram_char.transform(X_data)\n",
        "# assume that we don't have test set before\n",
        "X_test_tfidf_ngram_char =  tfidf_vect_ngram_char.transform(X_test)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4YLmX0p5tfk"
      },
      "source": [
        "#### Transform by SVD to decrease number of dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WUgthzR5tfk"
      },
      "source": [
        "##### Word Level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnK34xqN5tfk"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYywOfFj5tfl",
        "outputId": "89f6f3ba-30c1-41f7-f42e-5054e17ade2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "svd = TruncatedSVD(n_components=300, random_state=42)\n",
        "svd.fit(X_data_tfidf)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TruncatedSVD(algorithm='randomized', n_components=300, n_iter=5,\n",
              "             random_state=42, tol=0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh-OV_Dv5tfl"
      },
      "source": [
        "X_data_tfidf_svd = svd.transform(X_data_tfidf)\n",
        "X_test_tfidf_svd = svd.transform(X_test_tfidf)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dguZOIqt5tfm"
      },
      "source": [
        "##### ngram Level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uvC61875tfm",
        "outputId": "e4d2f8f6-dc86-4482-d7da-49763c65ae6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "svd_ngram = TruncatedSVD(n_components=300, random_state=42)\n",
        "svd_ngram.fit(X_data_tfidf_ngram)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TruncatedSVD(algorithm='randomized', n_components=300, n_iter=5,\n",
              "             random_state=42, tol=0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqfUXfz25tfn"
      },
      "source": [
        "X_data_tfidf_ngram_svd = svd_ngram.transform(X_data_tfidf_ngram)\n",
        "X_test_tfidf_ngram_svd = svd_ngram.transform(X_test_tfidf_ngram)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvakxx5d5tfn"
      },
      "source": [
        "##### ngram Char Level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NezEcMMl5tfn",
        "outputId": "79be4dee-335f-45c3-954a-67a0cbf103f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "svd_ngram_char = TruncatedSVD(n_components=300, random_state=42)\n",
        "svd_ngram_char.fit(X_data_tfidf_ngram_char)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TruncatedSVD(algorithm='randomized', n_components=300, n_iter=5,\n",
              "             random_state=42, tol=0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DustolYG5tfq"
      },
      "source": [
        "X_data_tfidf_ngram_char_svd = svd_ngram_char.transform(X_data_tfidf_ngram_char)\n",
        "X_test_tfidf_ngram_char_svd = svd_ngram_char.transform(X_test_tfidf_ngram_char)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ouPxz2_5tfq"
      },
      "source": [
        "### Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1iYBiHA5tfr"
      },
      "source": [
        "We will convert each word in document to a embedding vector. We will use pretrained model for Vietnamese. The model can be downloaded from https://github.com/Kyubyong/wordvectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyAscWY_5tfr"
      },
      "source": [
        "Assume that, one document have $n$ word, each word is represented by 300 dimensional vector, then the document vector be 2-dimensional matrix with size $ n \\times 300 $. From that, we can use DNN, RNN, CNN model for this type of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PCoiVMb5tfr"
      },
      "source": [
        "from gensim.models import KeyedVectors \n",
        "dir_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
        "\n",
        "!wget -c https://github.com/ltdaovn/Natual-Language-Processing/raw/master/vi.vec\n",
        "\n",
        "#word2vec_model_path = os.path.join(dir_path, \"Data/vi/vi.vec\")\n",
        "word2vec_model_path = \"vi.vec\"\n",
        "\n",
        "w2v = KeyedVectors.load_word2vec_format(word2vec_model_path)\n",
        "vocab = w2v.wv.vocab\n",
        "wv = w2v.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igWraeOW5tfs"
      },
      "source": [
        "def get_word2vec_data(X):\n",
        "    word2vec_data = []\n",
        "    for x in X:\n",
        "        sentence = []\n",
        "        for word in x.split(\" \"):\n",
        "            if word in vocab:\n",
        "#                 print(word)\n",
        "                sentence.append(wv[word])\n",
        "\n",
        "        word2vec_data.append(sentence)\n",
        "#         break\n",
        "    return word2vec_data\n",
        "\n",
        "X_data_w2v = get_word2vec_data(X_data)\n",
        "X_test_w2v = get_word2vec_data(X_test)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD5053P55tfs"
      },
      "source": [
        "\n",
        "\n",
        "### Text / NLP based features\n",
        "Idea from https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9ujnsuR5tfs"
      },
      "source": [
        "A number of extra text based features can also be created which sometimes are helpful for improving text classification models. Some examples are:\n",
        "\n",
        "1. Word Count of the documents – total number of words in the documents\n",
        "2. Character Count of the documents – total number of characters in the documents\n",
        "3. Average Word Density of the documents – average length of the words used in the documents\n",
        "4. Puncutation Count in the Complete Essay – total number of punctuation marks in the documents\n",
        "5. Upper Case Count in the Complete Essay – total number of upper count words in the documents\n",
        "6. Title Word Count in the Complete Essay – total number of proper case (title) words in the documents\n",
        "7. Frequency distribution of Part of Speech Tags:\n",
        "    - Noun Count\n",
        "    - Verb Count\n",
        "    - Adjective Count\n",
        "    - Adverb Count\n",
        "    - Pronoun Count\n",
        "    \n",
        "These features are highly experimental ones and should be used according to the problem statement only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmZrlONl5tft"
      },
      "source": [
        "### Topic Models as features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-3d2sbL5tft"
      },
      "source": [
        "Topic Modelling is a technique to identify the groups of words (called a topic) from a collection of documents that contains best information in the collection. I have used Latent Dirichlet Allocation for generating Topic Modelling Features. LDA is an iterative model which starts from a fixed number of topics. Each topic is represented as a distribution over words, and each document is then represented as a distribution over topics. Although the tokens themselves are meaningless, the probability distributions over words provided by the topics provide a sense of the different ideas contained in the documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMXTKub35tft"
      },
      "source": [
        "### Convert y to categorical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v1wuU705tft"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "y_data_n = encoder.fit_transform(y_data)\n",
        "y_test_n = encoder.fit_transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmq_p7hI5tft"
      },
      "source": [
        "encoder.classes_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1C5kDcD5tfu"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkqIfBqC5tfu"
      },
      "source": [
        "In this tutorial, we will implement some models and compare them to find the most effective model for text classification problem. We will implement these models:\n",
        "1. Naive Bayes Classifier\n",
        "2. Linear Classifier\n",
        "3. Support Vector Machine\n",
        "4. Bagging Models\n",
        "5. Boosting Models\n",
        "6. Shallow Neural Networks\n",
        "7. Deep Neural Networks\n",
        "    - Convolutional Neural Network (CNN)\n",
        "    - Long Short Term Modelr (LSTM)\n",
        "    - Gated Recurrent Unit (GRU)\n",
        "    - Bidirectional RNN\n",
        "    - Recurrent Convolutional Neural Network (RCNN)\n",
        "    - Other Variants of Deep Neural Networks\n",
        "8. Doc2Vec model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cF-caEo5tfu"
      },
      "source": [
        "We use the prototype function to do some classifiers as follows: <br>\n",
        "(Because of my machine memory, I test only on WORD LEVEL TF-IDF (with SVD or not))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoivG5mm5tfu"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUjJj6dB5tfv"
      },
      "source": [
        "def train_model(classifier, X_data, y_data, X_test, y_test, is_neuralnet=False, n_epochs=3):       \n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.1, random_state=42)\n",
        "    \n",
        "    if is_neuralnet:\n",
        "        classifier.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=n_epochs, batch_size=512)\n",
        "        \n",
        "        val_predictions = classifier.predict(X_val)\n",
        "        test_predictions = classifier.predict(X_test)\n",
        "        val_predictions = val_predictions.argmax(axis=-1)\n",
        "        test_predictions = test_predictions.argmax(axis=-1)\n",
        "    else:\n",
        "        classifier.fit(X_train, y_train)\n",
        "    \n",
        "        train_predictions = classifier.predict(X_train)\n",
        "        val_predictions = classifier.predict(X_val)\n",
        "        test_predictions = classifier.predict(X_test)\n",
        "        \n",
        "    print(\"Validation accuracy: \", metrics.accuracy_score(val_predictions, y_val))\n",
        "    print(\"Test accuracy: \", metrics.accuracy_score(test_predictions, y_test))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne2B5obv5tfv"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBaq9Yrm5tfv",
        "outputId": "e89b244e-e1bf-4717-9bfb-8298bb9a384e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_model(naive_bayes.MultinomialNB(), X_data_tfidf, y_data, X_test_tfidf, y_test, is_neuralnet=False)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy:  0.8640402843601895\n",
            "Test accuracy:  0.862942449328013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_l5Hvdj5tfx"
      },
      "source": [
        "train_model(naive_bayes.MultinomialNB(), X_data_tfidf_ngram_svd, y_data, X_test_tfidf_ngram_svd, y_test, is_neuralnet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQSxPX5Y5tfx"
      },
      "source": [
        "train_model(naive_bayes.MultinomialNB(), X_data_tfidf_ngram_char_svd, y_data, X_test_tfidf_ngram_char_svd, y_test, is_neuralnet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHHB6fuG5tfx"
      },
      "source": [
        "### Other type Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COGSegus5tfx"
      },
      "source": [
        "# use too much memory\n",
        "# train_model(naive_bayes.GaussianNB(), X_data_tfidf.todense(), y_data, X_test_tfidf.todense(), y_test, is_neuralnet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOiLsLg65tfy"
      },
      "source": [
        "train_model(naive_bayes.BernoulliNB(), X_data_tfidf, y_data, X_test_tfidf, y_test, is_neuralnet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1ZUxZMk5tfy"
      },
      "source": [
        "train_model(naive_bayes.BernoulliNB(), X_data_tfidf_svd, y_data, X_test_tfidf_svd, y_test, is_neuralnet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoU9EfM25tfy"
      },
      "source": [
        "## Linear Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z63CstQg5tfy"
      },
      "source": [
        "train_model(linear_model.LogisticRegression(), X_data_tfidf, y_data, X_test_tfidf, y_test, is_neuralnet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFqka-Bn5tfz"
      },
      "source": [
        "train_model(linear_model.LogisticRegression(), X_data_tfidf_svd, y_data, X_test_tfidf_svd, y_test, is_neuralnet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEVWliQO5tfz"
      },
      "source": [
        "## SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCaDYbJ25tfz"
      },
      "source": [
        "train_model(svm.SVC(), X_data_tfidf_svd, y_data, X_test_tfidf_svd, y_test, is_neuralnet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNyJfQ4O5tfz"
      },
      "source": [
        "## Bagging Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9DJAz535tfz"
      },
      "source": [
        "train_model(ensemble.RandomForestClassifier(), X_data_tfidf_svd, y_data, X_test_tfidf_svd, y_test, is_neuralnet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svg2eY_z5tf0"
      },
      "source": [
        "## Boosting Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baChNf1U5tf0"
      },
      "source": [
        "train_model(xgboost.XGBClassifier(), X_data_tfidf_svd, y_data, X_test_tfidf_svd, y_test, is_neuralnet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnV0eLrd5tf0"
      },
      "source": [
        "## Deep Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFwCD2qR5tf0"
      },
      "source": [
        "from keras.layers import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQB81-8q5tf0"
      },
      "source": [
        "def create_dnn_model():\n",
        "    input_layer = Input(shape=(300,))\n",
        "    layer = Dense(1024, activation='relu')(input_layer)\n",
        "    layer = Dense(1024, activation='relu')(layer)\n",
        "    layer = Dense(512, activation='relu')(layer)\n",
        "    output_layer = Dense(10, activation='softmax')(layer)\n",
        "    \n",
        "    classifier = models.Model(input_layer, output_layer)\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3E7YHDd5tf0"
      },
      "source": [
        "classifier = create_dnn_model()\n",
        "train_model(classifier=classifier, X_data=X_data_tfidf_svd, y_data=y_data_n, X_test=X_test_tfidf_svd, y_test=y_test_n, is_neuralnet=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIODq5ew5tf2"
      },
      "source": [
        "## Convolutional Neural Network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1MtCdNl5tf3"
      },
      "source": [
        "def create_cnn_model():\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2Q4SM0A5tf3"
      },
      "source": [
        "## Recurrent Neural Network  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gifk0kN5tf3"
      },
      "source": [
        "### LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QSVb9EX5tf3"
      },
      "source": [
        "def create_lstm_model():\n",
        "    input_layer = Input(shape=(300,))\n",
        "    \n",
        "    layer = Reshape((10, 30))(input_layer)\n",
        "    layer = LSTM(128, activation='relu')(layer)\n",
        "    layer = Dense(512, activation='relu')(layer)\n",
        "    layer = Dense(512, activation='relu')(layer)\n",
        "    layer = Dense(128, activation='relu')(layer)\n",
        "    \n",
        "    output_layer = Dense(10, activation='softmax')(layer)\n",
        "    \n",
        "    classifier = models.Model(input_layer, output_layer)\n",
        "    \n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HULj1zI5tf3"
      },
      "source": [
        "classifier = create_lstm_model()\n",
        "train_model(classifier=classifier, X_data=X_data_tfidf_svd, y_data=y_data_n, X_test=X_test_tfidf_svd, y_test=y_test_n, is_neuralnet=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiN31SnG5tf3"
      },
      "source": [
        "### GRU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65F5h8X55tf4"
      },
      "source": [
        "def create_gru_model():\n",
        "    input_layer = Input(shape=(300,))\n",
        "    \n",
        "    layer = Reshape((10, 30))(input_layer)\n",
        "    layer = GRU(128, activation='relu')(layer)\n",
        "    layer = Dense(512, activation='relu')(layer)\n",
        "    layer = Dense(512, activation='relu')(layer)\n",
        "    layer = Dense(128, activation='relu')(layer)\n",
        "    \n",
        "    output_layer = Dense(10, activation='softmax')(layer)\n",
        "    \n",
        "    classifier = models.Model(input_layer, output_layer)\n",
        "    \n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSpSv8Mk5tf4"
      },
      "source": [
        "classifier = create_gru_model()\n",
        "train_model(classifier=classifier, X_data=X_data_tfidf_svd, y_data=y_data_n, X_test=X_test_tfidf_svd, y_test=y_test_n, is_neuralnet=True, n_epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSD5Taqo5tf4"
      },
      "source": [
        "### Bidirectional RNN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54flnwKA5tf4"
      },
      "source": [
        "def create_brnn_model():\n",
        "    input_layer = Input(shape=(300,))\n",
        "    \n",
        "    layer = Reshape((10, 30))(input_layer)\n",
        "    layer = Bidirectional(GRU(128, activation='relu'))(layer)\n",
        "    layer = Dense(512, activation='relu')(layer)\n",
        "    layer = Dense(512, activation='relu')(layer)\n",
        "    layer = Dense(128, activation='relu')(layer)\n",
        "    \n",
        "    output_layer = Dense(10, activation='softmax')(layer)\n",
        "    \n",
        "    classifier = models.Model(input_layer, output_layer)\n",
        "    \n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S_yoMMx5tf4"
      },
      "source": [
        "classifier = create_brnn_model()\n",
        "train_model(classifier=classifier, X_data=X_data_tfidf_svd, y_data=y_data_n, X_test=X_test_tfidf_svd, y_test=y_test_n, is_neuralnet=True, n_epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4nfH3Iv5tf5"
      },
      "source": [
        "## Recurrent Convolutional Neural Network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtXX4Nms5tf5"
      },
      "source": [
        "# def create_rcnn_model():\n",
        "#     input_layer = Input(shape=(300,))\n",
        "    \n",
        "#     layer = Reshape((10, 30))(input_layer)\n",
        "#     layer = Bidirectional(GRU(128, activation='relu', return_sequences=True))(layer)\n",
        "# #     layer = Reshape((16, 16))(layer)\n",
        "# #     layer = Convolution1D(100, 3, activation=\"relu\")(layer)\n",
        "#     layer = Dense(512, activation='relu')(layer)\n",
        "#     layer = Dense(512, activation='relu')(layer)\n",
        "#     layer = Dense(128, activation='relu')(layer)\n",
        "    \n",
        "#     output_layer = Dense(10, activation='softmax')(layer)\n",
        "    \n",
        "#     classifier = models.Model(input_layer, output_layer)\n",
        "    \n",
        "#     classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "#     return classifier\n",
        "def create_rcnn_model():\n",
        "    input_layer = Input(shape=(300,))\n",
        "    \n",
        "    layer = Reshape((10, 30))(input_layer)\n",
        "    layer = Bidirectional(GRU(128, activation='relu', return_sequences=True))(layer)    \n",
        "    layer = Convolution1D(100, 3, activation=\"relu\")(layer)\n",
        "    layer = Flatten()(layer)\n",
        "    layer = Dense(512, activation='relu')(layer)\n",
        "    layer = Dense(512, activation='relu')(layer)\n",
        "    layer = Dense(128, activation='relu')(layer)\n",
        "    \n",
        "    output_layer = Dense(10, activation='softmax')(layer)\n",
        "    \n",
        "    classifier = models.Model(input_layer, output_layer)\n",
        "    classifier.summary()\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwhawO285tf5"
      },
      "source": [
        "classifier = create_rcnn_model()\n",
        "train_model(classifier=classifier, X_data=X_data_tfidf_svd, y_data=y_data_n, X_test=X_test_tfidf_svd, y_test=y_test_n, is_neuralnet=True, n_epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6zVe_bw5tf5"
      },
      "source": [
        "## Doc2Vec Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GudkZ7fy5tf6"
      },
      "source": [
        "def get_corpus(documents):\n",
        "    corpus = []\n",
        "    \n",
        "    for i in tqdm(range(len(documents))):\n",
        "        doc = documents[i]\n",
        "        \n",
        "        words = doc.split(' ')\n",
        "        tagged_document = gensim.models.doc2vec.TaggedDocument(words, [i])\n",
        "        \n",
        "        corpus.append(tagged_document)\n",
        "        \n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ybTcQ4z5tf6"
      },
      "source": [
        "train_corpus = get_corpus(X_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xALPin9U5tf6"
      },
      "source": [
        "test_corpus = get_corpus(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8kfHyFg5tf7"
      },
      "source": [
        "#### Build Doc2Vec model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqLsGdrv5tf7"
      },
      "source": [
        "model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=2, epochs=40)\n",
        "model.build_vocab(train_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQtaMTx55tf7"
      },
      "source": [
        "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHwAn1_X5tf7"
      },
      "source": [
        "#### Get vector "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mim4TlDy5tf7"
      },
      "source": [
        "X_data_vectors = []\n",
        "for x in train_corpus:\n",
        "    vector = model.infer_vector(x.words)\n",
        "    X_data_vectors.append(vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50mW16K75tf8"
      },
      "source": [
        "X_test_vectors = []\n",
        "for x in test_corpus:\n",
        "    vector = model.infer_vector(x.words)\n",
        "    X_test_vectors.append(vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "L9IF5HuF5tf8"
      },
      "source": [
        "classifier = create_dnn_model()\n",
        "train_model(classifier=classifier, X_data=np.array(X_data_vectors), y_data=y_data_n, X_test=(X_test_vectors), y_test=y_test_n, is_neuralnet=True, n_epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCacwA9v5tf8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}